# -*- coding: utf-8 -*-
"""CTGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1shIKxsXEFUb3reLjFKCPR6YwSDyKjIJZ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats
from torch.utils.data import DataLoader, TensorDataset
import os
from datetime import datetime

# Ensure reproducible results
torch.manual_seed(42)
np.random.seed(42)

if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)


from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt

torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

file_path = '/content/drive/MyDrive/imbalanced_dataset.csv'
# Specify the desired path
loaded_df = pd.read_csv(file_path)
print(loaded_df.shape)
print(loaded_df.head())
sns.countplot(x='zip_code', data=loaded_df)
sns.displot(loaded_df['transaction_amount'])
plt.show()

class TabularDataPreprocessor:
    """
    Data preprocessor implementing CTGAN-like mode-based normalization for continuous columns
    and standard one-hot encoding for categorical columns.

    This preprocessor also collects metadata for conditional generation.
    """
    def __init__(self, n_components_gmm=10):
        self.scalers = {}
        self.gmms = {}
        self.encoders = {}
        self.categorical_columns = []
        self.continuous_columns = []
        self.n_components_gmm = n_components_gmm
        self.is_fitted = False
        self.column_info = [] # (col_name, type, original_dim, transformed_dim_start, transformed_dim_end)

        # New: Metadata for conditional sampling
        self.categorical_transformed_info = [] # (col_name, start_idx, end_idx, one_hot_size, value_counts_normalized)

    def fit(self, data, categorical_columns=None):
        """Fit preprocessors and collect metadata."""
        self.categorical_columns = categorical_columns or []
        self.continuous_columns = [col for col in data.columns if col not in self.categorical_columns]

        current_transformed_dim = 0
        self.column_info = [] # Reset column_info

        for col in self.continuous_columns:
            scaler = StandardScaler()
            scaler.fit(data[col].values.reshape(-1, 1))
            self.scalers[col] = scaler

            gmm = GaussianMixture(n_components=self.n_components_gmm, random_state=42, covariance_type='full')
            gmm.fit(scaler.transform(data[col].values.reshape(-1, 1)))
            self.gmms[col] = gmm

            self.column_info.append((col, 'continuous', 1, current_transformed_dim, current_transformed_dim + self.n_components_gmm + 1))
            current_transformed_dim += (self.n_components_gmm + 1)

        # Collect categorical metadata for conditional sampling
        self.categorical_transformed_info = [] # Reset for refit
        for col in self.categorical_columns:
            encoder = LabelEncoder()
            encoder.fit(data[col].astype(str))
            self.encoders[col] = encoder
            n_categories = len(self.encoders[col].classes_)

            self.column_info.append((col, 'categorical', 1, current_transformed_dim, current_transformed_dim + n_categories))

            # Store info for conditional sampling: name, start_idx, end_idx, num_categories, normalized frequencies
            normalized_counts = data[col].value_counts(normalize=True).reindex(encoder.classes_, fill_value=0)
            self.categorical_transformed_info.append({
                'name': col,
                'start_idx': current_transformed_dim,
                'end_idx': current_transformed_dim + n_categories,
                'num_categories': n_categories,
                'proportions': normalized_counts.values
            })
            current_transformed_dim += n_categories

        self.is_fitted = True
        return self

    def transform(self, data):
        """Transform data to network-friendly format using mode-based approach for continuous."""
        if not self.is_fitted:
            raise ValueError("Preprocessor not fitted yet! Call .fit() first.")

        transformed_data_parts = []
        for col_name, col_type, _, _, _ in self.column_info: # Iterate based on fitted order
            if col_type == 'continuous':
                scaler = self.scalers[col_name]
                gmm = self.gmms[col_name]

                scaled_data = scaler.transform(data[col_name].values.reshape(-1, 1))
                gmm_components = gmm.predict(scaled_data) # Use predict for hard assignment
                gmm_means = gmm.means_[gmm_components]
                gmm_covariances = gmm.covariances_[gmm_components]
                gmm_stds = np.sqrt(gmm_covariances).reshape(-1, 1)
                gmm_stds[gmm_stds == 0] = 1e-6
                normalized_value_in_component = (scaled_data - gmm_means) / gmm_stds

                one_hot_component = np.zeros((len(data), self.n_components_gmm))
                one_hot_component[np.arange(len(data)), gmm_components] = 1

                transformed_data_parts.append(one_hot_component)
                transformed_data_parts.append(normalized_value_in_component)

            elif col_type == 'categorical':
                encoder = self.encoders[col_name]
                encoded = encoder.transform(data[col_name].astype(str))
                n_categories = len(encoder.classes_)
                one_hot = np.zeros((len(encoded), n_categories))
                one_hot[np.arange(len(encoded)), encoded] = 1
                transformed_data_parts.append(one_hot)

        return np.hstack(transformed_data_parts)

    def inverse_transform(self, transformed_data):
        """Convert transformed data back to original format."""
        if not self.is_fitted:
            raise ValueError("Preprocessor not fitted yet! Call .fit() first.")

        reconstructed_data = {}
        current_idx = 0

        for col_name, col_type, original_dim, start_idx, end_idx in self.column_info:
            if col_type == 'continuous':
                component_one_hot = transformed_data[:, current_idx : current_idx + self.n_components_gmm]
                normalized_value_in_component = transformed_data[:, current_idx + self.n_components_gmm : current_idx + self.n_components_gmm + 1]

                gmm_components_pred = np.argmax(component_one_hot, axis=1)

                scaler = self.scalers[col_name]
                gmm = self.gmms[col_name]

                gmm_means = gmm.means_[gmm_components_pred].reshape(-1,1)
                gmm_covariances = gmm.covariances_[gmm_components_pred].reshape(-1,1)
                gmm_stds = np.sqrt(gmm_covariances)
                gmm_stds[gmm_stds == 0] = 1e-6

                scaled_original_value = (normalized_value_in_component * gmm_stds) + gmm_means
                original_col = scaler.inverse_transform(scaled_original_value)
                reconstructed_data[col_name] = original_col.flatten()
                current_idx += (self.n_components_gmm + 1)

            elif col_type == 'categorical':
                n_categories = len(self.encoders[col_name].classes_)
                one_hot_cols = transformed_data[:, current_idx : current_idx + n_categories]
                category_indices = np.argmax(one_hot_cols, axis=1)
                original_categories = self.encoders[col_name].inverse_transform(category_indices)
                reconstructed_data[col_name] = original_categories
                current_idx += n_categories

        return pd.DataFrame(reconstructed_data)

    def get_output_dim(self):
        """Get total dimension after preprocessing."""
        dim = 0
        for col_name, col_type, _, _, _ in self.column_info:
            if col_type == 'continuous':
                dim += (self.n_components_gmm + 1)
            elif col_type == 'categorical':
                dim += len(self.encoders[col_name].classes_)
        return dim

    def get_conditional_dim(self):
        """Get total dimension of the conditional vector."""
        # The conditional vector will be one-hot for one selected categorical feature at a time
        # So its dimension is the max number of categories in any categorical column
        if not self.categorical_transformed_info:
            return 0
        return sum(info['num_categories'] for info in self.categorical_transformed_info)

# --- Helper Module: GumbelSoftmax ---
class GumbelSoftmax(nn.Module):
    def __init__(self, temperature=1.0, hard=False):
        super(GumbelSoftmax, self).__init__()
        self.temperature = temperature
        self.hard = hard

    def forward(self, logits):
        # Gumbel-Softmax trick: G = -log(-log(U))
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)
        y = logits + gumbel_noise
        y = torch.softmax(y / self.temperature, dim=-1)

        if self.hard:
            # For hard (one-hot) output during inference
            y_hard = torch.zeros_like(y)
            y_hard.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.0)
            # Straight-through estimator
            y = y_hard - y.detach() + y
        return y

# --- CTGANGenerator ---
class CTGANGenerator(nn.Module):
    """CTGAN Generator for tabular data with conditional input and Gumbel-Softmax output."""

    def __init__(self, noise_dim, data_dim, cond_dim, hidden_dims=[128, 256, 128]):
        super(CTGANGenerator, self).__init__()

        self.data_dim = data_dim
        self.cond_dim = cond_dim

        layers = []
        prev_dim = noise_dim + cond_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, data_dim))

        self.network = nn.Sequential(*layers)
        self.gumbel_softmax = GumbelSoftmax(temperature=0.7, hard=True)

    def forward(self, noise, cond_vector, column_info):
        """
        Generates synthetic data conditioned on cond_vector.

        Args:
            noise: Random noise vector
            cond_vector: One-hot encoded conditional vector
            column_info: Metadata from preprocessor
        """
        combined_input = torch.cat((noise, cond_vector), dim=1)
        logits = self.network(combined_input)

        output_data = torch.empty_like(logits)
        for col_name, col_type, _, start_idx, end_idx in column_info:
            if col_type == 'continuous':
                output_data[:, start_idx:end_idx] = torch.tanh(logits[:, start_idx:end_idx])
            elif col_type == 'categorical':
                output_data[:, start_idx:end_idx] = self.gumbel_softmax(logits[:, start_idx:end_idx])

        return output_data

# --- CTGANCritic ---
class CTGANCritic(nn.Module):
    """CTGAN Critic (Discriminator) for tabular data with conditional input."""

    def __init__(self, data_dim, cond_dim, hidden_dims=[128, 256, 128]):
        super(CTGANCritic, self).__init__()

        layers = []
        prev_dim = data_dim + cond_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x, cond_vector):
        """Evaluates data conditioned on cond_vector."""
        combined_input = torch.cat((x, cond_vector), dim=1)
        return self.network(combined_input)

# --- CTGAN Main Class ---
class CTGAN:
    """CTGAN implementation for tabular data with conditional generation."""

    def __init__(self, preprocessor, noise_dim=100, lr=2e-4, lambda_gp=10, critic_iter=5):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # GPU memory check for Colab
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"GPU Available: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {gpu_memory:.1f} GB")
            torch.cuda.empty_cache()

        self.lambda_gp = lambda_gp
        self.critic_iter = critic_iter
        self.preprocessor = preprocessor
        self.noise_dim = noise_dim

        # Get dimensions after preprocessor is fitted
        self.data_dim = self.preprocessor.get_output_dim()
        self.cond_dim = self.preprocessor.get_conditional_dim()

        # Validate dimensions
        if self.data_dim == 0:
            raise ValueError("Preprocessor not fitted or no data columns found")

        print(f"Data dimension: {self.data_dim}")
        print(f"Conditional dimension: {self.cond_dim}")

        # Initialize networks
        self.generator = CTGANGenerator(self.noise_dim, self.data_dim, self.cond_dim).to(self.device)
        self.critic = CTGANCritic(self.data_dim, self.cond_dim).to(self.device)

        # Optimizers
        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.9))
        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=lr, betas=(0.5, 0.9))

        # Store categorical info
        self.categorical_transformed_info = preprocessor.categorical_transformed_info
        self.num_categorical_cols = len(self.categorical_transformed_info)

    def _sample_conditional_vector(self, batch_size):
        """Samples conditional vector for a batch based on CTGAN strategy."""
        if not self.categorical_transformed_info:
            return torch.zeros(batch_size, self.cond_dim, device=self.device), None

        # Calculate total conditional dimension
        total_cond_dim = sum(info['num_categories'] for info in self.categorical_transformed_info)

        if total_cond_dim != self.cond_dim:
            raise ValueError(f"Conditional dimension mismatch: {total_cond_dim} vs {self.cond_dim}")

        cond_vector_batch = torch.zeros(batch_size, total_cond_dim, device=self.device)
        column_masks = torch.zeros(batch_size, self.data_dim, dtype=torch.bool, device=self.device)

        for i in range(batch_size):
            # Randomly select a categorical column
            chosen_col_info_idx = np.random.randint(0, self.num_categorical_cols)
            chosen_col_info = self.categorical_transformed_info[chosen_col_info_idx]

            # Get proportions for this column and re-normalize for sampling
            proportions = chosen_col_info['proportions']
            proportions_sum = np.sum(proportions)
            if proportions_sum == 0:
                # Handle case where all proportions are zero (shouldn't happen with correct fitting but as a safeguard)
                normalized_proportions = np.ones(len(proportions)) / len(proportions)
            else:
                normalized_proportions = proportions / proportions_sum


            # Sample a category from this column
            sampled_category_idx = np.random.choice(
                a=np.arange(chosen_col_info['num_categories']),
                p=normalized_proportions # Use normalized probabilities
            )

            # Calculate global index in conditional vector
            global_cond_start_idx = 0
            for k in range(chosen_col_info_idx):
                global_cond_start_idx += self.categorical_transformed_info[k]['num_categories']

            # Set the one-hot
            cond_vector_batch[i, global_cond_start_idx + sampled_category_idx] = 1.0

            # Create mask for generator loss
            column_masks[i, :] = True
            column_masks[i, chosen_col_info['start_idx']:chosen_col_info['end_idx']] = False

        return cond_vector_batch, column_masks

    def gradient_penalty(self, real_data, fake_data, cond_vector):
        """Calculates gradient penalty for WGAN-GP with conditional input."""
        batch_size = real_data.size(0)
        epsilon = torch.rand(batch_size, 1, device=self.device)

        interpolated = epsilon * real_data + (1 - epsilon) * fake_data
        interpolated.requires_grad_(True)

        critic_scores = self.critic(interpolated, cond_vector)

        gradients = torch.autograd.grad(
            outputs=critic_scores,
            inputs=interpolated,
            grad_outputs=torch.ones_like(critic_scores, device=self.device),
            create_graph=True,
            retain_graph=True
        )[0]

        gradients = gradients.view(batch_size, -1)
        gradient_norm = gradients.norm(2, dim=1)
        penalty = torch.mean((gradient_norm - 1) ** 2)

        return penalty

    def train_step(self, real_data_batch):
        """Performs a single training step for CTGAN."""
        batch_size = real_data_batch.size(0)
        real_data_batch = real_data_batch.to(self.device)

        # Sample conditional vector and mask
        cond_vector, column_mask_for_gen_loss = self._sample_conditional_vector(batch_size)

        # Train Critic
        for _ in range(self.critic_iter):
            noise = torch.randn(batch_size, self.noise_dim, device=self.device)
            fake_data = self.generator(noise, cond_vector, self.preprocessor.column_info)

            real_score = self.critic(real_data_batch, cond_vector)
            fake_score = self.critic(fake_data.detach(), cond_vector)

            gp = self.gradient_penalty(real_data_batch, fake_data, cond_vector)
            c_loss = -real_score.mean() + fake_score.mean() + self.lambda_gp * gp

            self.c_optimizer.zero_grad()
            c_loss.backward()
            self.c_optimizer.step()

        # Train Generator
        noise = torch.randn(batch_size, self.noise_dim, device=self.device)
        fake_data = self.generator(noise, cond_vector, self.preprocessor.column_info)
        fake_score = self.critic(fake_data, cond_vector)

        g_loss = -fake_score.mean()

        self.g_optimizer.zero_grad()
        g_loss.backward()
        self.g_optimizer.step()

        return {
            'critic_loss': c_loss.item(),
            'generator_loss': g_loss.item(),
            'gradient_penalty': gp.item()
        }

    def generate_samples(self, n_samples):
        """Generate synthetic samples using the trained generator."""
        self.generator.eval()
        all_synthetic_samples = []

        if not self.preprocessor.categorical_transformed_info:
            print("No categorical columns for conditional generation. Generating unconditioned samples.")
            with torch.no_grad():
                noise = torch.randn(n_samples, self.noise_dim, device=self.device)
                dummy_cond_vector = torch.zeros(n_samples, self.cond_dim, device=self.device)
                fake_data = self.generator(noise, dummy_cond_vector, self.preprocessor.column_info)
            all_synthetic_samples.append(fake_data.cpu().numpy())
        else:
            first_categorical_col_info = self.preprocessor.categorical_transformed_info[0]
            num_categories_in_first_col = first_categorical_col_info['num_categories']

            print(f"Generating samples conditioned on each of the {num_categories_in_first_col} categories in '{first_categorical_col_info['name']}'...")

            for cat_idx in range(num_categories_in_first_col):
                with torch.no_grad():
                    noise = torch.randn(n_samples, self.noise_dim, device=self.device)

                    cond_vector_for_cat = torch.zeros(n_samples, self.cond_dim, device=self.device)
                    global_cond_start_idx = 0
                    for k in range(len(self.preprocessor.categorical_transformed_info)):
                        if k == 0: # Assuming the first categorical column is used for conditioning
                             cond_vector_for_cat[:, global_cond_start_idx + cat_idx] = 1.0
                             break
                        global_cond_start_idx += self.preprocessor.categorical_transformed_info[k]['num_categories']


                    fake_data = self.generator(noise, cond_vector_for_cat, self.preprocessor.column_info)
                    all_synthetic_samples.append(fake_data.cpu().numpy())

        self.generator.train()
        return np.vstack(all_synthetic_samples)

    def save_model(self, path):
        """Saves the model state for CTGAN."""
        torch.save({
            'generator_state_dict': self.generator.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'g_optimizer_state_dict': self.g_optimizer.state_dict(),
            'c_optimizer_state_dict': self.c_optimizer.state_dict(),
            'noise_dim': self.noise_dim,
            'data_dim': self.data_dim,
            'cond_dim': self.cond_dim,
            'preprocessor_state': self.preprocessor
        }, path)
        print(f"CTGAN model state saved to: {path}")

    @classmethod
    def load_model(cls, path, lr=2e-4, lambda_gp=10, critic_iter=5):
        """Loads a CTGAN model from saved state."""
        checkpoint = torch.load(path, map_location=torch.device('cpu'))

        preprocessor = checkpoint['preprocessor_state']
        noise_dim = checkpoint['noise_dim']

        loaded_ctgan = cls(preprocessor, noise_dim, lr, lambda_gp, critic_iter)

        loaded_ctgan.generator.load_state_dict(checkpoint['generator_state_dict'])
        loaded_ctgan.critic.load_state_dict(checkpoint['critic_state_dict'])
        loaded_ctgan.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])
        loaded_ctgan.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])

        return loaded_ctgan

# --- Training Function Optimized for Colab ---
def train_ctgan_colab(data, preprocessor, epochs=1000, batch_size=64, save_every=200):
    """Training function optimized for Colab GPU constraints."""

    # Transform data
    transformed_data = preprocessor.transform(data)

    # Create CTGAN
    ctgan = CTGAN(preprocessor, noise_dim=100, lr=2e-4)

    # Prepare data loader
    tensor_data = torch.tensor(transformed_data, dtype=torch.float32)
    dataset = TensorDataset(tensor_data)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop with GPU monitoring
    print("Starting training...")
    for epoch in range(epochs):
        epoch_losses = {'critic': [], 'generator': [], 'gp': []}

        for batch_idx, (batch_data,) in enumerate(dataloader):
            losses = ctgan.train_step(batch_data)

            for key in epoch_losses:
                epoch_losses[key].append(losses[f'{key}_loss' if key != 'gp' else 'gradient_penalty'])

            # GPU memory check every 100 batches
            if batch_idx % 100 == 0 and torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                if memory_used > 10:  # Warning if using more than 10GB
                    print(f"Warning: High GPU memory usage: {memory_used:.2f} GB")
                    torch.cuda.empty_cache()

        # Print progress
        if epoch % 100 == 0:
            avg_losses = {key: np.mean(vals) for key, vals in epoch_losses.items()}
            print(f"Epoch {epoch}: Critic Loss: {avg_losses['critic']:.4f}, "
                  f"Generator Loss: {avg_losses['generator']:.4f}, "
                  f"GP: {avg_losses['gp']:.4f}")

        # Save checkpoint
        if epoch % save_every == 0 and epoch > 0:
            ctgan.save_model(f'/content/drive/MyDrive/ctgan_checkpoint_epoch_{epoch}.pth')
            print(f"Checkpoint saved at epoch {epoch}")

    print("Training completed!")
    return ctgan

# --- Usage Example ---

loaded_df.columns

# 1. First, create and fit your TabularDataPreprocessor
preprocessor = TabularDataPreprocessor(n_components_gmm=10)
preprocessor.fit(loaded_df, categorical_columns=['zip_code', 'sic_code','us_state'])

    # 2. Train the CTGAN
ctgan_model = train_ctgan_colab(loaded_df, preprocessor, epochs=1000)

    # 3. Generate synthetic data
synthetic_data = ctgan_model.generate_samples(1000)

    # 4. Convert back to original format
synthetic_df = preprocessor.inverse_transform(synthetic_data)

#     # 5. Save the model
# ctgan_model.save_model('my_ctgan_model.pth')

#     # 6. Load the model later

loaded_model = CTGAN.load_model('ctgan_checkpoint_epoch_500.pth')

