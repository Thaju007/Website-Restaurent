"""
Enterprise-ready CTGAN implementation using functional programming approach
Based on the CTGAN paper with PyTorch implementation
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from sklearn.preprocessing import OneHotEncoder

# Constants
BATCH_SIZE = 500
CRITIC_ITERS = 5
LAMBDA_GP = 10
LAMBDA_COND = 1.0
Z_DIM = 128
HIDDEN_DIM = 256
LEARNING_RATE = 2e-4
EPOCHS = 300

# --------- Preprocessing Functions ---------

def fit_gaussian_mixture_models(data, continuous_columns):
    """Fit Gaussian Mixture Models to continuous columns for mode-specific normalization"""
    gmms = {}
    n_components = {}
    
    for column in continuous_columns:
        # Step 1: Use VGM to estimate number of modes
        vgm = BayesianGaussianMixture(
            n_components=10,  # Start with a high number and let VGM prune
            weight_concentration_prior_type="dirichlet_process",
            weight_concentration_prior=0.001,
            n_init=5,
            random_state=42
        )
        vgm.fit(data[column].values.reshape(-1, 1))
        
        # Count effective components (those with non-negligible weights)
        effective_components = np.sum(vgm.weights_ > 0.01)
        effective_components = max(1, effective_components)  # At least 1 component
        n_components[column] = effective_components
        
        # Step 2: Fit a regular GMM with the estimated number of components
        gmm = GaussianMixture(
            n_components=effective_components,
            n_init=5,
            random_state=42
        )
        gmm.fit(data[column].values.reshape(-1, 1))
        gmms[column] = gmm
    
    return gmms, n_components

def normalize_continuous_columns(data, continuous_columns, gmms):
    """
    Apply mode-specific normalization to continuous columns
    Returns normalized data and mode indicators
    """
    normalized_data = data.copy()
    mode_indicators = {}
    
    for column in continuous_columns:
        gmm = gmms[column]
        values = data[column].values.reshape(-1, 1)
        
        # Step 3: Compute probability densities for each mode
        probs = gmm.predict_proba(values)
        
        # Step 4: Sample modes based on these probabilities
        modes = np.zeros_like(probs)
        selected_modes = np.argmax(probs, axis=1)
        modes[np.arange(len(selected_modes)), selected_modes] = 1
        
        # Normalize within the selected mode
        means = gmm.means_.reshape(-1)
        stds = np.sqrt(gmm.covariances_).reshape(-1)
        
        normalized_values = np.zeros(len(values))
        for mode_idx in range(gmm.n_components):
            mode_filter = (selected_modes == mode_idx)
            if np.any(mode_filter):
                normalized_values[mode_filter] = ((values[mode_filter].reshape(-1) - means[mode_idx]) / 
                                                 (4 * stds[mode_idx]))
        
        normalized_data[column] = normalized_values
        mode_indicators[column] = modes
    
    return normalized_data, mode_indicators

def fit_categorical_encoders(data, categorical_columns):
    """Fit one-hot encoders for categorical columns"""
    encoders = {}
    encoded_dims = {}
    
    for column in categorical_columns:
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        encoder.fit(data[column].values.reshape(-1, 1))
        encoders[column] = encoder
        encoded_dims[column] = len(encoder.categories_[0])
    
    return encoders, encoded_dims

def transform_data(data, continuous_columns, categorical_columns, gmms, encoders):
    """Transform data for model input"""
    # Normalize continuous columns
    normalized_data, mode_indicators = normalize_continuous_columns(data, continuous_columns, gmms)
    
    # Transform categorical columns using one-hot encoding
    transformed_data = []
    
    # First, add normalized continuous data
    for column in continuous_columns:
        transformed_data.append(normalized_data[column].values.reshape(-1, 1))
    
    # Next, add mode indicators for continuous data
    for column in continuous_columns:
        transformed_data.append(mode_indicators[column])
    
    # Finally, add one-hot encoded categorical data
    for column in categorical_columns:
        encoder = encoders[column]
        encoded = encoder.transform(data[column].values.reshape(-1, 1))
        transformed_data.append(encoded)
    
    # Concatenate all components
    return np.hstack(transformed_data)

def inverse_transform(generated_data, continuous_columns, categorical_columns, gmms, encoders, n_continuous):
    """Convert generated data back to original format"""
    data_dict = {}
    
    # Process continuous columns
    current_idx = 0
    cont_normalized_data = {}
    mode_indicators = {}
    
    # Extract normalized values
    for i, column in enumerate(continuous_columns):
        cont_normalized_data[column] = generated_data[:, current_idx].numpy()
        current_idx += 1
    
    # Extract mode indicators
    for i, column in enumerate(continuous_columns):
        gmm = gmms[column]
        n_modes = gmm.n_components
        mode_indicators[column] = generated_data[:, current_idx:current_idx + n_modes].numpy()
        current_idx += n_modes
    
    # Denormalize continuous data
    for column in continuous_columns:
        gmm = gmms[column]
        normalized_values = cont_normalized_data[column]
        modes = np.argmax(mode_indicators[column], axis=1)
        
        means = gmm.means_.reshape(-1)
        stds = np.sqrt(gmm.covariances_).reshape(-1)
        
        denormalized = np.zeros_like(normalized_values)
        for mode_idx in range(gmm.n_components):
            mode_filter = (modes == mode_idx)
            if np.any(mode_filter):
                denormalized[mode_filter] = (normalized_values[mode_filter] * 4 * stds[mode_idx]) + means[mode_idx]
        
        data_dict[column] = denormalized
    
    # Process categorical columns
    for column in categorical_columns:
        encoder = encoders[column]
        n_categories = len(encoder.categories_[0])
        
        # Extract one-hot encoded data for this column
        encoded_data = generated_data[:, current_idx:current_idx + n_categories].numpy()
        current_idx += n_categories
        
        # Convert from one-hot back to categorical
        categorical_data = np.argmax(encoded_data, axis=1)
        
        # Map from numeric indices back to original categories
        original_categories = encoder.categories_[0]
        data_dict[column] = np.array([original_categories[idx] for idx in categorical_data])
    
    return pd.DataFrame(data_dict)

# --------- GAN Model Functions ---------

def gumbel_softmax(logits, temperature=0.2):
    """Gumbel-Softmax activation function"""
    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)
    y = logits + gumbel_noise
    return F.softmax(y / temperature, dim=-1)

def generator_network(z, cond, continuous_dims, mode_sizes, categorical_dims):
    """
    Generator network as described in the paper
    z: random noise
    cond: conditional vector for controlling discrete outputs
    """
    # Combine noise and condition
    h0 = torch.cat([z, cond], dim=1)
    input_dim = z.size(1) + cond.size(1)
    
    # First hidden layer with skip connection
    fc1 = nn.Linear(input_dim, HIDDEN_DIM)
    bn1 = nn.BatchNorm1d(HIDDEN_DIM)
    h1 = torch.cat([h0, F.relu(bn1(fc1(h0)))], dim=1)
    
    # Second hidden layer with skip connection
    fc2 = nn.Linear(input_dim + HIDDEN_DIM, HIDDEN_DIM)
    bn2 = nn.BatchNorm1d(HIDDEN_DIM)
    h2 = torch.cat([h1, F.relu(bn2(fc2(h1)))], dim=1)
    
    last_dim = input_dim + HIDDEN_DIM * 2
    
    # Output layers
    output_parts = []
    
    # Generate continuous normalized values using tanh
    for i in range(continuous_dims):
        fc_alpha = nn.Linear(last_dim, 1)
        alpha = torch.tanh(fc_alpha(h2))
        output_parts.append(alpha)
    
    # Generate mode indicators using gumbel softmax
    for size in mode_sizes:
        fc_beta = nn.Linear(last_dim, size)
        beta = gumbel_softmax(fc_beta(h2), temperature=0.2)
        output_parts.append(beta)
    
    # Generate discrete values using gumbel softmax
    for size in categorical_dims:
        fc_d = nn.Linear(last_dim, size)
        d = gumbel_softmax(fc_d(h2), temperature=0.2)
        output_parts.append(d)
    
    # Concatenate all outputs
    return torch.cat(output_parts, dim=1)

def critic_network(x, cond, pac_size=10):
    """
    Critic network as described in the paper
    Implements PacGAN with given pac_size
    """
    # Reshape inputs for pac
    batch_size = x.size(0) // pac_size
    x = x.view(batch_size, pac_size * x.size(1))
    cond = cond.view(batch_size, pac_size * cond.size(1))
    
    # Combine inputs and condition
    h0 = torch.cat([x, cond], dim=1)
    
    # First hidden layer with dropout and leaky ReLU
    fc1 = nn.Linear(h0.size(1), HIDDEN_DIM)
    h1 = F.leaky_relu(fc1(h0), 0.2)
    h1 = F.dropout(h1, 0.5, training=True)
    
    # Second hidden layer with dropout and leaky ReLU
    fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)
    h2 = F.leaky_relu(fc2(h1), 0.2)
    h2 = F.dropout(h2, 0.5, training=True)
    
    # Output layer
    fc3 = nn.Linear(HIDDEN_DIM, 1)
    return fc3(h2)

def compute_gradient_penalty(critic, real_data, fake_data, cond, device):
    """Calculate gradient penalty for WGAN-GP"""
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, device=device)
    
    # Interpolate between real and fake data
    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)
    
    # Calculate critic output for interpolated data
    disc_interpolates = critic_network(interpolates, cond)
    
    # Calculate gradients
    gradients = torch.autograd.grad(
        outputs=disc_interpolates, 
        inputs=interpolates,
        grad_outputs=torch.ones_like(disc_interpolates),
        create_graph=True, 
        retain_graph=True
    )[0]
    
    # Calculate gradient penalty
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()
    
    return gradient_penalty

def sample_training_data(data_loader, categorical_dims, device):
    """Sample training data with training-by-sampling method"""
    # Get a batch of real data
    try:
        real_data, real_cond = next(data_loader_iter)
    except:
        data_loader_iter = iter(data_loader)
        real_data, real_cond = next(data_loader_iter)
    
    real_data = real_data.to(device)
    real_cond = real_cond.to(device)
    
    # Create conditional vector for generator
    batch_size = real_data.size(0)
    cond = torch.zeros_like(real_cond)
    
    # For each sample, randomly select one discrete column
    for i in range(batch_size):
        # Randomly select a categorical column
        if len(categorical_dims) > 0:
            selected_col = np.random.randint(len(categorical_dims))
            
            # Create probability distribution based on log(frequency)
            # For simplicity, we use uniform sampling here
            # In a full implementation, you'd use the actual frequencies
            col_start = sum(categorical_dims[:selected_col])
            col_size = categorical_dims[selected_col]
            col_val = np.random.randint(col_size)
            
            # Set the corresponding element to 1
            cond[i, col_start + col_val] = 1
    
    return real_data, real_cond, cond

def create_noise(batch_size, z_dim, device):
    """Create random noise for generator input"""
    return torch.randn(batch_size, z_dim, device=device)

def train_ctgan(data, continuous_columns, categorical_columns, device='cuda'):
    """Train the CTGAN model"""
    # Preprocess data
    gmms, n_components = fit_gaussian_mixture_models(data, continuous_columns)
    encoders, encoded_dims = fit_categorical_encoders(data, categorical_columns)
    
    transformed_data = transform_data(data, continuous_columns, categorical_columns, gmms, encoders)
    
    # Prepare dimensions information
    continuous_dims = len(continuous_columns)
    mode_sizes = [n_components[col] for col in continuous_columns]
    categorical_dims = [encoded_dims[col] for col in categorical_columns]
    
    # Calculate conditional vector size (sum of all categorical dimensions)
    cond_dim = sum(categorical_dims)
    
    # Create dataset and dataloader
    tensor_x = torch.Tensor(transformed_data)
    
    # Create conditional vectors (one-hot encoding of categorical columns)
    cond_data = np.zeros((len(tensor_x), cond_dim))
    current_pos = 0
    for i, col in enumerate(categorical_columns):
        col_data = transformed_data[:, -sum(categorical_dims) + current_pos:
                                    -sum(categorical_dims) + current_pos + categorical_dims[i]]
        cond_data[:, current_pos:current_pos + categorical_dims[i]] = col_data
        current_pos += categorical_dims[i]
    
    tensor_cond = torch.Tensor(cond_data)
    dataset = TensorDataset(tensor_x, tensor_cond)
    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    data_loader_iter = iter(data_loader)
    
    # Store optimizers and modules in dicts to ensure proper tracking
    gen_modules = {}
    critic_modules = {}
    
    # Generator modules
    gen_modules['fc1'] = nn.Linear(Z_DIM + cond_dim, HIDDEN_DIM).to(device)
    gen_modules['bn1'] = nn.BatchNorm1d(HIDDEN_DIM).to(device)
    gen_modules['fc2'] = nn.Linear(Z_DIM + cond_dim + HIDDEN_DIM, HIDDEN_DIM).to(device)
    gen_modules['bn2'] = nn.BatchNorm1d(HIDDEN_DIM).to(device)
    
    last_dim = Z_DIM + cond_dim + HIDDEN_DIM * 2
    
    # Output layers for generator
    for i in range(continuous_dims):
        gen_modules[f'fc_alpha_{i}'] = nn.Linear(last_dim, 1).to(device)
    
    for i, size in enumerate(mode_sizes):
        gen_modules[f'fc_beta_{i}'] = nn.Linear(last_dim, size).to(device)
    
    for i, size in enumerate(categorical_dims):
        gen_modules[f'fc_d_{i}'] = nn.Linear(last_dim, size).to(device)
    
    # Critic modules
    input_dim = tensor_x.size(1)
    critic_modules['fc1'] = nn.Linear(10 * input_dim + 10 * cond_dim, HIDDEN_DIM).to(device)
    critic_modules['fc2'] = nn.Linear(HIDDEN_DIM, HIDDEN_DIM).to(device)
    critic_modules['fc3'] = nn.Linear(HIDDEN_DIM, 1).to(device)
    
    # Collect parameters for optimizers
    generator_parameters = []
    for module in gen_modules.values():
        generator_parameters.extend(module.parameters())
    
    critic_parameters = []
    for module in critic_modules.values():
        critic_parameters.extend(module.parameters())
    
    # Create optimizers with proper learning rates
    optimizer_g = optim.Adam(generator_parameters, lr=LEARNING_RATE, betas=(0.5, 0.9))
    optimizer_c = optim.Adam(critic_parameters, lr=LEARNING_RATE, betas=(0.5, 0.9))
    
    # Define a generator function that uses the modules
    def generator_forward(z, cond):
        # Combine noise and condition
        h0 = torch.cat([z, cond], dim=1)
        
        # First hidden layer with skip connection
        h1_part = F.relu(gen_modules['bn1'](gen_modules['fc1'](h0)))
        h1 = torch.cat([h0, h1_part], dim=1)
        
        # Second hidden layer with skip connection
        h2_part = F.relu(gen_modules['bn2'](gen_modules['fc2'](h1)))
        h2 = torch.cat([h1, h2_part], dim=1)
        
        # Output layers
        output_parts = []
        
        # Generate continuous normalized values using tanh
        for i in range(continuous_dims):
            alpha = torch.tanh(gen_modules[f'fc_alpha_{i}'](h2))
            output_parts.append(alpha)
        
        # Generate mode indicators using gumbel softmax
        for i, size in enumerate(mode_sizes):
            beta = gumbel_softmax(gen_modules[f'fc_beta_{i}'](h2), temperature=0.2)
            output_parts.append(beta)
        
        # Generate discrete values using gumbel softmax
        for i, size in enumerate(categorical_dims):
            d = gumbel_softmax(gen_modules[f'fc_d_{i}'](h2), temperature=0.2)
            output_parts.append(d)
        
        # Concatenate all outputs
        return torch.cat(output_parts, dim=1)
    
    # Define a critic function that uses the modules
    def critic_forward(x, cond, pac_size=10):
        # Reshape inputs for pac
        batch_size = x.size(0) // pac_size
        x = x.view(batch_size, pac_size * x.size(1))
        cond = cond.view(batch_size, pac_size * cond.size(1))
        
        # Combine inputs and condition
        h0 = torch.cat([x, cond], dim=1)
        
        # First hidden layer with dropout and leaky ReLU
        h1 = F.leaky_relu(critic_modules['fc1'](h0), 0.2)
        h1 = F.dropout(h1, 0.5, training=True)
        
        # Second hidden layer with dropout and leaky ReLU
        h2 = F.leaky_relu(critic_modules['fc2'](h1), 0.2)
        h2 = F.dropout(h2, 0.5, training=True)
        
        # Output layer
        return critic_modules['fc3'](h2)
    
    # Function to compute gradient penalty
    def compute_gradient_penalty(real_data, fake_data, cond, device):
        batch_size = real_data.size(0)
        alpha = torch.rand(batch_size, 1, device=device)
        
        # Interpolate between real and fake data
        alpha = alpha.expand(real_data.size())
        interpolates = alpha * real_data + (1 - alpha) * fake_data
        interpolates.requires_grad_(True)
        
        # Calculate critic output for interpolated data
        disc_interpolates = critic_forward(interpolates, cond)
        
        # Calculate gradients
        gradients = torch.autograd.grad(
            outputs=disc_interpolates, 
            inputs=interpolates,
            grad_outputs=torch.ones_like(disc_interpolates, device=device),
            create_graph=True, 
            retain_graph=True
        )[0]
        
        # Calculate gradient penalty
        gradients = gradients.view(batch_size, -1)
        gradient_norm = gradients.norm(2, dim=1)
        gradient_penalty = ((gradient_norm - 1) ** 2).mean()
        
        return gradient_penalty
    
    # Modified training-by-sampling function
    def sample_training_data(data_loader, data_loader_iter, categorical_dims, device):
        # Get a batch of real data
        try:
            real_data, real_cond = next(data_loader_iter)
        except StopIteration:
            data_loader_iter = iter(data_loader)
            real_data, real_cond = next(data_loader_iter)
        
        real_data = real_data.to(device)
        real_cond = real_cond.to(device)
        
        # Create conditional vector for generator
        batch_size = real_data.size(0)
        cond = torch.zeros_like(real_cond)
        
        # For each sample, randomly select one discrete column
        if len(categorical_dims) > 0:
            for i in range(batch_size):
                # Randomly select a categorical column
                selected_col = np.random.randint(len(categorical_dims))
                
                # Create probability distribution based on log(frequency)
                # For simplicity, we use uniform sampling here
                col_start = sum(categorical_dims[:selected_col])
                col_size = categorical_dims[selected_col]
                col_val = np.random.randint(col_size)
                
                # Set the corresponding element to 1
                cond[i, col_start + col_val] = 1
        
        return real_data, real_cond, cond, data_loader_iter
    
    # Initialize tracking variables
    critic_losses = []
    generator_losses = []
    
    # Apply weight initialization
    for module in gen_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0.0)
    
    for module in critic_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0.0)
    
    # Training loop with stability improvements
    for epoch in range(EPOCHS):
        # Track epoch losses
        epoch_critic_losses = []
        epoch_generator_losses = []
        
        for i in range(len(data_loader)):
            # Train critic
            for _ in range(CRITIC_ITERS):
                # Sample real data
                real_data, real_cond, cond, data_loader_iter = sample_training_data(
                    data_loader, data_loader_iter, categorical_dims, device)
                
                # Generate fake data
                z = torch.randn(BATCH_SIZE, Z_DIM, device=device)
                with torch.no_grad():  # Don't track gradients for generator during critic training
                    fake_data = generator_forward(z, cond)
                
                # Calculate critic loss
                real_critic = critic_forward(real_data, real_cond)
                fake_critic = critic_forward(fake_data, cond)
                
                # Gradient penalty
                gp = compute_gradient_penalty(real_data, fake_data, cond, device)
                
                # Wasserstein loss with stability term
                critic_loss = torch.mean(fake_critic) - torch.mean(real_critic) + LAMBDA_GP * gp
                
                # Update critic
                optimizer_c.zero_grad()
                critic_loss.backward()
                
                # Apply gradient clipping to critic
                torch.nn.utils.clip_grad_norm_(critic_parameters, max_norm=10.0)
                optimizer_c.step()
                
                epoch_critic_losses.append(critic_loss.item())
            
            # Train generator
            # Sample conditions and generate fake data
            _, _, cond, data_loader_iter = sample_training_data(
                data_loader, data_loader_iter, categorical_dims, device)
            z = torch.randn(BATCH_SIZE, Z_DIM, device=device)
            fake_data = generator_forward(z, cond)
            
            # Calculate generator loss
            fake_critic = critic_forward(fake_data, cond)
            generator_loss = -torch.mean(fake_critic)
            
            # Add conditional loss if categorical columns exist
            if cond_dim > 0:
                # Extract the generated categorical part
                cat_start_idx = continuous_dims + sum(mode_sizes)
                cat_fake_data = fake_data[:, cat_start_idx:]
                
                # Calculate cross-entropy loss for each categorical column
                cond_loss = 0
                current_pos = 0
                for i, dim in enumerate(categorical_dims):
                    target = cond[:, current_pos:current_pos + dim]
                    pred = cat_fake_data[:, current_pos:current_pos + dim]
                    
                    # Cross entropy between target and prediction
                    cond_loss += F.binary_cross_entropy(pred + 1e-8, target + 1e-8)
                    current_pos += dim
                
                generator_loss += LAMBDA_COND * cond_loss
            
            # Update generator
            optimizer_g.zero_grad()
            generator_loss.backward()
            
            # Apply gradient clipping to generator
            torch.nn.utils.clip_grad_norm_(generator_parameters, max_norm=10.0)
            optimizer_g.step()
            
            epoch_generator_losses.append(generator_loss.item())
        
        # Calculate average losses for the epoch
        avg_critic_loss = sum(epoch_critic_losses) / len(epoch_critic_losses) if epoch_critic_losses else 0
        avg_generator_loss = sum(epoch_generator_losses) / len(epoch_generator_losses) if epoch_generator_losses else 0
        
        critic_losses.append(avg_critic_loss)
        generator_losses.append(avg_generator_loss)
        
        # Print progress
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS}, G Loss: {avg_generator_loss:.4f}, C Loss: {avg_critic_loss:.4f}")
            
            # Check for NaN values
            if np.isnan(avg_generator_loss) or np.isnan(avg_critic_loss):
                print("NaN values detected - training has diverged. Stopping training.")
                break
            
            # Check if losses are improving
            if epoch > 50 and epoch % 50 == 0:
                # Check if generator loss is improving
                recent_g_loss = np.mean(generator_losses[-20:])
                earlier_g_loss = np.mean(generator_losses[-50:-30])
                
                if abs(recent_g_loss - earlier_g_loss) < 0.01:
                    print("Generator loss has plateaued. Consider adjusting learning rate or model parameters.")
                
                # Check if critic loss is improving
                recent_c_loss = np.mean(critic_losses[-20:])
                earlier_c_loss = np.mean(critic_losses[-50:-30])
                
                if abs(recent_c_loss - earlier_c_loss) < 0.01:
                    print("Critic loss has plateaued. Consider adjusting learning rate or model parameters.")
    
    # Create a model dictionary to return
    model = {
        'gmms': gmms,
        'n_components': n_components,
        'encoders': encoders,
        'encoded_dims': encoded_dims,
        'continuous_columns': continuous_columns,
        'categorical_columns': categorical_columns,
        'continuous_dims': continuous_dims,
        'mode_sizes': mode_sizes,
        'categorical_dims': categorical_dims,
        'cond_dim': cond_dim,
        'gen_modules': gen_modules,
        'generator_forward': generator_forward,
        'z_dim': Z_DIM,
        'device': device
    }
    
    return model, critic_losses, generator_losses

def generate_samples(model, n_samples):
    """Generate synthetic samples using the trained model"""
    # Unpack model components
    gmms = model['gmms']
    encoders = model['encoders']
    continuous_columns = model['continuous_columns']
    categorical_columns = model['categorical_columns']
    continuous_dims = model['continuous_dims']
    mode_sizes = model['mode_sizes']
    categorical_dims = model['categorical_dims']
    cond_dim = model['cond_dim']
    gen_modules = model['gen_modules']
    generator_forward = model['generator_forward']
    z_dim = model['z_dim']
    device = model['device']
    
    # Create random noise
    z = torch.randn(n_samples, z_dim, device=device)
    
    # Create random conditions (for enterprise use, you may want to control this)
    cond = torch.zeros(n_samples, cond_dim, device=device)
    if len(categorical_dims) > 0:
        for i in range(n_samples):
            for j, dim in enumerate(categorical_dims):
                col_start = sum(categorical_dims[:j])
                col_val = np.random.randint(dim)
                cond[i, col_start + col_val] = 1
    
    # Generate data
    with torch.no_grad():
        fake_data = generator_forward(z, cond)
    
    # Convert to original format
    return inverse_transform(fake_data, continuous_columns, categorical_columns, 
                           gmms, encoders, continuous_dims)

# Example usage
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("your_data.csv")
    
    # Define continuous and categorical columns
    continuous_columns = ["age", "income", "height", "weight"]
    categorical_columns = ["gender", "education", "occupation"]
    
    # Train model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, critic_losses, generator_losses = train_ctgan(data, continuous_columns, categorical_columns, device)
    
    # Plot training losses
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 5))
    plt.plot(critic_losses, label='Critic Loss')
    plt.plot(generator_losses, label='Generator Loss')
    plt.legend()
    plt.title('CTGAN Training Losses')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig('training_losses.png')
    
    # Generate synthetic data
    synthetic_data = generate_samples(model, 1000)
    
    # Save synthetic data
    synthetic_data.to_csv("synthetic_data.csv", index=False)
































##new changes
# Add these imports for date handling
from datetime import datetime, timedelta
import random

# --------- Date Processing Functions ---------

def process_date_columns(data, date_columns):
    """Process date columns into numerical features"""
    processed_data = data.copy()
    date_encoders = {}
    
    for column in date_columns:
        # Convert to datetime if not already
        if not pd.api.types.is_datetime64_any_dtype(data[column]):
            processed_data[column] = pd.to_datetime(data[column], errors='coerce')
        
        # Extract useful date components
        col_prefix = f"{column}_"
        processed_data[col_prefix + 'year'] = processed_data[column].dt.year
        processed_data[col_prefix + 'month'] = processed_data[column].dt.month
        processed_data[col_prefix + 'day'] = processed_data[column].dt.day
        processed_data[col_prefix + 'dayofweek'] = processed_data[column].dt.dayofweek
        
        # Store reference date and min/max for reconstruction
        min_date = processed_data[column].min()
        max_date = processed_data[column].max()
        date_encoders[column] = {
            'min_date': min_date,
            'max_date': max_date,
            'components': ['year', 'month', 'day', 'dayofweek']
        }
        
        # Drop original column
        processed_data.drop(column, axis=1, inplace=True)
    
    # Get all the new continuous columns
    date_continuous_columns = []
    for column in date_columns:
        col_prefix = f"{column}_"
        date_continuous_columns.extend([
            col_prefix + 'year', 
            col_prefix + 'month', 
            col_prefix + 'day'
        ])
    
    return processed_data, date_encoders, date_continuous_columns

def reconstruct_date_columns(data, date_encoders):
    """Reconstruct date columns from numerical features"""
    reconstructed_data = data.copy()
    
    for column, encoder in date_encoders.items():
        col_prefix = f"{column}_"
        
        # Create datetime from components
        years = reconstructed_data[col_prefix + 'year'].astype(int)
        months = reconstructed_data[col_prefix + 'month'].astype(int).clip(1, 12)
        days = reconstructed_data[col_prefix + 'day'].astype(int).clip(1, 28)  # Safer with 28
        
        # Reconstruct dates
        reconstructed_dates = []
        for year, month, day in zip(years, months, days):
            try:
                date = datetime(year=year, month=month, day=day)
                reconstructed_dates.append(date)
            except ValueError:
                # Handle invalid dates (e.g., Feb 30) by using a valid date in the same month
                try:
                    date = datetime(year=year, month=month, day=1)
                    reconstructed_dates.append(date)
                except ValueError:
                    # Fallback to reference date
                    reconstructed_dates.append(encoder['min_date'])
        
        reconstructed_data[column] = reconstructed_dates
        
        # Drop component columns
        for component in encoder['components']:
            reconstructed_data.drop(col_prefix + component, axis=1, inplace=True)
    
    return reconstructed_data




def train_ctgan(data, continuous_columns, categorical_columns, date_columns=None, 
               device='cuda', batch_size=500, critic_iters=5, lambda_gp=10, 
               lambda_cond=1.0, z_dim=128, hidden_dim=256, learning_rate=2e-4, 
               epochs=300, weight_decay=1e-5, temperature=0.2):
    """
    Train the CTGAN model with improved stability for more categorical variables
    Added date column support and regularization
    """
    # Process date columns if provided
    date_encoders = None
    date_continuous_columns = []
    if date_columns and len(date_columns) > 0:
        data, date_encoders, date_continuous_columns = process_date_columns(data, date_columns)
        continuous_columns = continuous_columns + date_continuous_columns
    
    # Preprocess data
    gmms, n_components = fit_gaussian_mixture_models(data, continuous_columns)
    encoders, encoded_dims = fit_categorical_encoders(data, categorical_columns)
    
    transformed_data = transform_data(data, continuous_columns, categorical_columns, gmms, encoders)
    
    # Prepare dimensions information
    continuous_dims = len(continuous_columns)
    mode_sizes = [n_components[col] for col in continuous_columns]
    categorical_dims = [encoded_dims[col] for col in categorical_columns]
    
    # Calculate conditional vector size (sum of all categorical dimensions)
    cond_dim = sum(categorical_dims)
    
    # Create dataset and dataloader
    tensor_x = torch.Tensor(transformed_data)
    
    # Create conditional vectors (one-hot encoding of categorical columns)
    cond_data = np.zeros((len(tensor_x), cond_dim))
    current_pos = 0
    for i, col in enumerate(categorical_columns):
        col_data = transformed_data[:, -sum(categorical_dims) + current_pos:
                                   -sum(categorical_dims) + current_pos + categorical_dims[i]]
        cond_data[:, current_pos:current_pos + categorical_dims[i]] = col_data
        current_pos += categorical_dims[i]
    
    tensor_cond = torch.Tensor(cond_data)
    dataset = TensorDataset(tensor_x, tensor_cond)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    data_loader_iter = iter(data_loader)
    
    # Store optimizers and modules in dicts to ensure proper tracking
    gen_modules = {}
    critic_modules = {}
    
    # Determine adaptive sizes based on number of categorical variables
    adaptive_hidden_dim = hidden_dim
    if len(categorical_columns) > 5:
        # Increase network capacity for many categorical variables
        adaptive_hidden_dim = hidden_dim * 2
    
    # Generator modules with adaptive capacity and better initialization
    gen_modules['fc1'] = nn.Linear(z_dim + cond_dim, adaptive_hidden_dim).to(device)
    gen_modules['bn1'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
    gen_modules['fc2'] = nn.Linear(z_dim + cond_dim + adaptive_hidden_dim, adaptive_hidden_dim).to(device)
    gen_modules['bn2'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
    
    # Add an additional layer for complexity when many categorical variables
    if len(categorical_columns) > 7:
        gen_modules['fc3'] = nn.Linear(z_dim + cond_dim + adaptive_hidden_dim * 2, adaptive_hidden_dim).to(device)
        gen_modules['bn3'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
        last_dim = z_dim + cond_dim + adaptive_hidden_dim * 3
    else:
        last_dim = z_dim + cond_dim + adaptive_hidden_dim * 2
    
    # Output layers for generator
    for i in range(continuous_dims):
        gen_modules[f'fc_alpha_{i}'] = nn.Linear(last_dim, 1).to(device)
    
    for i, size in enumerate(mode_sizes):
        gen_modules[f'fc_beta_{i}'] = nn.Linear(last_dim, size).to(device)
    
    for i, size in enumerate(categorical_dims):
        gen_modules[f'fc_d_{i}'] = nn.Linear(last_dim, size).to(device)
    
    # Critic modules with increased capacity
    input_dim = tensor_x.size(1)
    critic_modules['fc1'] = nn.Linear(10 * input_dim + 10 * cond_dim, adaptive_hidden_dim).to(device)
    critic_modules['fc2'] = nn.Linear(adaptive_hidden_dim, adaptive_hidden_dim).to(device)
    
    # Add an additional layer for critic when many categorical variables
    if len(categorical_columns) > 7:
        critic_modules['fc3'] = nn.Linear(adaptive_hidden_dim, adaptive_hidden_dim).to(device)
        critic_modules['fc_out'] = nn.Linear(adaptive_hidden_dim, 1).to(device)
    else:
        critic_modules['fc_out'] = nn.Linear(adaptive_hidden_dim, 1).to(device)
    
    # Collect parameters for optimizers
    generator_parameters = []
    for module in gen_modules.values():
        generator_parameters.extend(module.parameters())
    
    critic_parameters = []
    for module in critic_modules.values():
        critic_parameters.extend(module.parameters())
    
    # Create optimizers with proper learning rates and weight decay for regularization
    optimizer_g = optim.Adam(generator_parameters, lr=learning_rate, betas=(0.5, 0.9), weight_decay=weight_decay)
    optimizer_c = optim.Adam(critic_parameters, lr=learning_rate, betas=(0.5, 0.9), weight_decay=weight_decay)
    
    # Define generator function with many categorical variables support
    def generator_forward(z, cond):
        # Combine noise and condition
        h0 = torch.cat([z, cond], dim=1)
        
        # First hidden layer with skip connection
        h1_part = F.relu(gen_modules['bn1'](gen_modules['fc1'](h0)))
        h1 = torch.cat([h0, h1_part], dim=1)
        
        # Second hidden layer with skip connection
        h2_part = F.relu(gen_modules['bn2'](gen_modules['fc2'](h1)))
        h2 = torch.cat([h1, h2_part], dim=1)
        
        # Third hidden layer with skip connection if needed
        if len(categorical_columns) > 7:
            h3_part = F.relu(gen_modules['bn3'](gen_modules['fc3'](h2)))
            h3 = torch.cat([h2, h3_part], dim=1)
            final_h = h3
        else:
            final_h = h2
        
        # Output layers
        output_parts = []
        
        # Generate continuous normalized values using tanh
        for i in range(continuous_dims):
            alpha = torch.tanh(gen_modules[f'fc_alpha_{i}'](final_h))
            output_parts.append(alpha)
        
        # Generate mode indicators using gumbel softmax
        for i, size in enumerate(mode_sizes):
            beta = gumbel_softmax(gen_modules[f'fc_beta_{i}'](final_h), temperature=temperature)
            output_parts.append(beta)
        
        # Generate discrete values using gumbel softmax
        for i, size in enumerate(categorical_dims):
            d = gumbel_softmax(gen_modules[f'fc_d_{i}'](final_h), temperature=temperature)
            output_parts.append(d)
        
        # Concatenate all outputs
        return torch.cat(output_parts, dim=1)
    
    # Define critic function with many categorical variables support
    def critic_forward(x, cond, pac_size=10):
        # Reshape inputs for pac
        batch_size = x.size(0) // pac_size
        x = x.view(batch_size, pac_size * x.size(1))
        cond = cond.view(batch_size, pac_size * cond.size(1))
        
        # Combine inputs and condition
        h0 = torch.cat([x, cond], dim=1)
        
        # First hidden layer with dropout and leaky ReLU
        h1 = F.leaky_relu(critic_modules['fc1'](h0), 0.2)
        h1 = F.dropout(h1, 0.5, training=True)
        
        # Second hidden layer with dropout and leaky ReLU
        h2 = F.leaky_relu(critic_modules['fc2'](h1), 0.2)
        h2 = F.dropout(h2, 0.5, training=True)
        
        # Third hidden layer if needed
        if len(categorical_columns) > 7:
            h3 = F.leaky_relu(critic_modules['fc3'](h2), 0.2)
            h3 = F.dropout(h3, 0.5, training=True)
            final_h = h3
        else:
            final_h = h2
        
        # Output layer
        return critic_modules['fc_out'](final_h)




def generate_samples(model, n_samples, with_date_columns=False):
    """Generate synthetic samples using the trained model with date column support"""
    # Unpack model components
    gmms = model['gmms']
    encoders = model['encoders']
    continuous_columns = model['continuous_columns']
    categorical_columns = model['categorical_columns']
    continuous_dims = model['continuous_dims']
    mode_sizes = model['mode_sizes']
    categorical_dims = model['categorical_dims']
    cond_dim = model['cond_dim']
    gen_modules = model['gen_modules']
    generator_forward = model['generator_forward']
    z_dim = model['z_dim']
    device = model['device']
    date_encoders = model.get('date_encoders', None)
    
    # Create random noise
    z = torch.randn(n_samples, z_dim, device=device)
    
    # Create random conditions 
    cond = torch.zeros(n_samples, cond_dim, device=device)
    if len(categorical_dims) > 0:
        for i in range(n_samples):
            for j, dim in enumerate(categorical_dims):
                col_start = sum(categorical_dims[:j])
                col_val = np.random.randint(dim)
                cond[i, col_start + col_val] = 1
    
    # Generate data
    with torch.no_grad():
        fake_data = generator_forward(z, cond)
    
    # Convert to original format
    result = inverse_transform(fake_data, continuous_columns, categorical_columns, 
                           gmms, encoders, continuous_dims)
    
    # Reconstruct date columns if necessary
    if with_date_columns and date_encoders:
        result = reconstruct_date_columns(result, date_encoders)
    
    return result



def initialize_weights(model_dict):
    """Apply orthogonal initialization to model weights for better training stability"""
    for module_name, module in model_dict.items():
        if isinstance(module, nn.Linear):
            # Orthogonal initialization for weights
            nn.init.orthogonal_(module.weight, gain=1.0)
            # Zero initialization for bias
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)

def create_lr_scheduler(optimizer, epochs):
    """Create learning rate scheduler for better convergence"""
    return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)



def train_ctgan_with_dates(data, continuous_columns, categorical_columns, date_columns=None, 
                      device='cuda', batch_size=500, critic_iters=5, lambda_gp=10, 
                      lambda_cond=1.0, z_dim=128, hidden_dim=256, learning_rate=2e-4, 
                      epochs=300, weight_decay=1e-5, temperature=0.2):
    """Improved CTGAN training with date support and enhanced stability"""
    # Process date columns if provided
    date_encoders = None
    date_continuous_columns = []
    if date_columns and len(date_columns) > 0:
        data, date_encoders, date_continuous_columns = process_date_columns(data, date_columns)
        continuous_columns = continuous_columns + date_continuous_columns
    
    # Rest of the code as in train_ctgan, but with additional initializations and schedulers
    # ...
    
    # After creating optimizers, add schedulers
    scheduler_g = create_lr_scheduler(optimizer_g, epochs)
    scheduler_c = create_lr_scheduler(optimizer_c, epochs)
    
    # Initialize weights for better stability
    initialize_weights(gen_modules)
    initialize_weights(critic_modules)
    
    # Modify the training loop to use schedulers and handle many categorical variables
    # ...
    
    # At the end of each epoch, update the learning rate
    scheduler_g.step()
    scheduler_c.step()
    
    # Add date_encoders to the model dictionary
    model = {
        # ... existing keys ...
        'date_encoders': date_encoders
    }
    
    return model, critic_losses, generator_losses



# Example usage
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("your_data.csv")
    
    # Define columns types
    continuous_columns = ["age", "income", "height", "weight"]
    categorical_columns = ["gender", "education", "occupation", "marital_status", "region", "sector"]
    date_columns = ["birth_date", "join_date"]
    
    # Train model with date support
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, critic_losses, generator_losses = train_ctgan_with_dates(
        data, continuous_columns, categorical_columns, date_columns=date_columns, 
        device=device, epochs=300, learning_rate=1e-4, weight_decay=1e-5)
    
    # Generate synthetic data with date columns
    synthetic_data = generate_samples(model, 1000, with_date_columns=True)
    
    # Save synthetic data
    synthetic_data.to_csv("synthetic_data.csv", index=False)




def train_ctgan(data, continuous_columns, categorical_columns, date_columns=None, 
               device='cuda', batch_size=500, critic_iters=5, lambda_gp=10, 
               lambda_cond=1.0, z_dim=128, hidden_dim=256, learning_rate=2e-4, 
               epochs=300, weight_decay=1e-5, temperature=0.2):
    """
    Train the CTGAN model with improved stability for more categorical variables
    Added date column support and regularization
    """
    # Process date columns if provided
    date_encoders = None
    date_continuous_columns = []
    if date_columns and len(date_columns) > 0:
        data, date_encoders, date_continuous_columns = process_date_columns(data, date_columns)
        continuous_columns = continuous_columns + date_continuous_columns
    
    # Preprocess data
    gmms, n_components = fit_gaussian_mixture_models(data, continuous_columns)
    encoders, encoded_dims = fit_categorical_encoders(data, categorical_columns)
    
    transformed_data = transform_data(data, continuous_columns, categorical_columns, gmms, encoders)
    
    # Prepare dimensions information
    continuous_dims = len(continuous_columns)
    mode_sizes = [n_components[col] for col in continuous_columns]
    categorical_dims = [encoded_dims[col] for col in categorical_columns]
    
    # Calculate conditional vector size (sum of all categorical dimensions)
    cond_dim = sum(categorical_dims)
    
    # Create dataset and dataloader
    tensor_x = torch.Tensor(transformed_data)
    
    # Create conditional vectors (one-hot encoding of categorical columns)
    cond_data = np.zeros((len(tensor_x), cond_dim))
    current_pos = 0
    for i, col in enumerate(categorical_columns):
        col_data = transformed_data[:, -sum(categorical_dims) + current_pos:
                                   -sum(categorical_dims) + current_pos + categorical_dims[i]]
        cond_data[:, current_pos:current_pos + categorical_dims[i]] = col_data
        current_pos += categorical_dims[i]
    
    tensor_cond = torch.Tensor(cond_data)
    dataset = TensorDataset(tensor_x, tensor_cond)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    data_loader_iter = iter(data_loader)
    
    # Store optimizers and modules in dicts to ensure proper tracking
    gen_modules = {}
    critic_modules = {}
    
    # Determine adaptive sizes based on number of categorical variables
    adaptive_hidden_dim = hidden_dim
    if len(categorical_columns) > 5:
        # Increase network capacity for many categorical variables
        adaptive_hidden_dim = hidden_dim * 2
    
    # Generator modules with adaptive capacity
    gen_modules['fc1'] = nn.Linear(z_dim + cond_dim, adaptive_hidden_dim).to(device)
    gen_modules['bn1'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
    gen_modules['fc2'] = nn.Linear(z_dim + cond_dim + adaptive_hidden_dim, adaptive_hidden_dim).to(device)
    gen_modules['bn2'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
    
    # Add an additional layer for complexity when many categorical variables
    if len(categorical_columns) > 7:
        gen_modules['fc3'] = nn.Linear(z_dim + cond_dim + adaptive_hidden_dim * 2, adaptive_hidden_dim).to(device)
        gen_modules['bn3'] = nn.BatchNorm1d(adaptive_hidden_dim).to(device)
        last_dim = z_dim + cond_dim + adaptive_hidden_dim * 3
    else:
        last_dim = z_dim + cond_dim + adaptive_hidden_dim * 2
    
    # Output layers for generator
    for i in range(continuous_dims):
        gen_modules[f'fc_alpha_{i}'] = nn.Linear(last_dim, 1).to(device)
    
    for i, size in enumerate(mode_sizes):
        gen_modules[f'fc_beta_{i}'] = nn.Linear(last_dim, size).to(device)
    
    for i, size in enumerate(categorical_dims):
        gen_modules[f'fc_d_{i}'] = nn.Linear(last_dim, size).to(device)
    
    # Critic modules with increased capacity
    input_dim = tensor_x.size(1)
    critic_modules['fc1'] = nn.Linear(10 * input_dim + 10 * cond_dim, adaptive_hidden_dim).to(device)
    critic_modules['fc2'] = nn.Linear(adaptive_hidden_dim, adaptive_hidden_dim).to(device)
    
    # Add an additional layer for critic when many categorical variables
    if len(categorical_columns) > 7:
        critic_modules['fc3'] = nn.Linear(adaptive_hidden_dim, adaptive_hidden_dim).to(device)
        critic_modules['fc_out'] = nn.Linear(adaptive_hidden_dim, 1).to(device)
    else:
        critic_modules['fc_out'] = nn.Linear(adaptive_hidden_dim, 1).to(device)
    
    # Apply orthogonal initialization to model weights for better training stability
    for module in gen_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=1.0)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
    
    for module in critic_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=1.0)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
    
    # Collect parameters for optimizers
    generator_parameters = []
    for module in gen_modules.values():
        generator_parameters.extend(module.parameters())
    
    critic_parameters = []
    for module in critic_modules.values():
        critic_parameters.extend(module.parameters())
    
    # Create optimizers with proper learning rates and weight decay for regularization
    optimizer_g = optim.Adam(generator_parameters, lr=learning_rate, betas=(0.5, 0.9), weight_decay=weight_decay)
    optimizer_c = optim.Adam(critic_parameters, lr=learning_rate, betas=(0.5, 0.9), weight_decay=weight_decay)
    
    # Create learning rate schedulers for better convergence
    scheduler_g = optim.lr_scheduler.CosineAnnealingLR(optimizer_g, T_max=epochs, eta_min=learning_rate/10)
    scheduler_c = optim.lr_scheduler.CosineAnnealingLR(optimizer_c, T_max=epochs, eta_min=learning_rate/10)
    
    # Define generator function with many categorical variables support
    def generator_forward(z, cond):
        # Combine noise and condition
        h0 = torch.cat([z, cond], dim=1)
        
        # First hidden layer with skip connection
        h1_part = F.relu(gen_modules['bn1'](gen_modules['fc1'](h0)))
        h1 = torch.cat([h0, h1_part], dim=1)
        
        # Second hidden layer with skip connection
        h2_part = F.relu(gen_modules['bn2'](gen_modules['fc2'](h1)))
        h2 = torch.cat([h1, h2_part], dim=1)
        
        # Third hidden layer with skip connection if needed
        if len(categorical_columns) > 7:
            h3_part = F.relu(gen_modules['bn3'](gen_modules['fc3'](h2)))
            h3 = torch.cat([h2, h3_part], dim=1)
            final_h = h3
        else:
            final_h = h2
        
        # Output layers
        output_parts = []
        
        # Generate continuous normalized values using tanh
        for i in range(continuous_dims):
            alpha = torch.tanh(gen_modules[f'fc_alpha_{i}'](final_h))
            output_parts.append(alpha)
        
        # Generate mode indicators using gumbel softmax
        for i, size in enumerate(mode_sizes):
            beta = gumbel_softmax(gen_modules[f'fc_beta_{i}'](final_h), temperature=temperature)
            output_parts.append(beta)
        
        # Generate discrete values using gumbel softmax
        for i, size in enumerate(categorical_dims):
            d = gumbel_softmax(gen_modules[f'fc_d_{i}'](final_h), temperature=temperature)
            output_parts.append(d)
        
        # Concatenate all outputs
        return torch.cat(output_parts, dim=1)
    
    # Define critic function with many categorical variables support
    def critic_forward(x, cond, pac_size=10):
        # Reshape inputs for pac
        batch_size = x.size(0) // pac_size
        x = x.view(batch_size, pac_size * x.size(1))
        cond = cond.view(batch_size, pac_size * cond.size(1))
        
        # Combine inputs and condition
        h0 = torch.cat([x, cond], dim=1)
        
        # First hidden layer with dropout and leaky ReLU
        h1 = F.leaky_relu(critic_modules['fc1'](h0), 0.2)
        h1 = F.dropout(h1, 0.5, training=True)
        
        # Second hidden layer with dropout and leaky ReLU
        h2 = F.leaky_relu(critic_modules['fc2'](h1), 0.2)
        h2 = F.dropout(h2, 0.5, training=True)
        
        # Third hidden layer if needed
        if len(categorical_columns) > 7:
            h3 = F.leaky_relu(critic_modules['fc3'](h2), 0.2)
            h3 = F.dropout(h3, 0.5, training=True)
            final_h = h3
        else:
            final_h = h2
        
        # Output layer
        return critic_modules['fc_out'](final_h)
    
    # Function to compute gradient penalty with improved stability
    def compute_gradient_penalty(real_data, fake_data, cond, device):
        batch_size = real_data.size(0)
        alpha = torch.rand(batch_size, 1, device=device)
        
        # Interpolate between real and fake data
        alpha = alpha.expand(real_data.size())
        interpolates = alpha * real_data + (1 - alpha) * fake_data
        interpolates.requires_grad_(True)
        
        # Calculate critic output for interpolated data
        disc_interpolates = critic_forward(interpolates, cond)
        
        # Calculate gradients
        gradients = torch.autograd.grad(
            outputs=disc_interpolates, 
            inputs=interpolates,
            grad_outputs=torch.ones_like(disc_interpolates, device=device),
            create_graph=True, 
            retain_graph=True
        )[0]
        
        # Calculate gradient penalty with epsilon for numerical stability
        gradients = gradients.view(batch_size, -1)
        gradient_norm = gradients.norm(2, dim=1)
        gradient_penalty = ((gradient_norm - 1) ** 2).mean()
        
        return gradient_penalty
    
    # Modified training-by-sampling function with better balancing for many categorical variables
    def sample_training_data(data_loader, data_loader_iter, categorical_dims, device):
        # Get a batch of real data
        try:
            real_data, real_cond = next(data_loader_iter)
        except StopIteration:
            data_loader_iter = iter(data_loader)
            real_data, real_cond = next(data_loader_iter)
        
        real_data = real_data.to(device)
        real_cond = real_cond.to(device)
        
        # Create conditional vector for generator
        batch_size = real_data.size(0)
        cond = torch.zeros_like(real_cond)
        
        # For each sample, randomly select categorical columns with improved balancing
        if len(categorical_dims) > 0:
            # Probability of selecting each categorical column - biased toward columns with more categories
            # This helps balance the learning for many categorical variables
            col_probs = np.array(categorical_dims) / sum(categorical_dims)
            
            for i in range(batch_size):
                # Select categorical columns based on their dimensionality
                selected_col = np.random.choice(len(categorical_dims), p=col_probs)
                
                col_start = sum(categorical_dims[:selected_col])
                col_size = categorical_dims[selected_col]
                
                # Balance sampling of values within a column
                col_val = np.random.randint(col_size)
                
                # Set the corresponding element to 1
                cond[i, col_start + col_val] = 1
        
        return real_data, real_cond, cond, data_loader_iter
    
    # Initialize tracking variables
    critic_losses = []
    generator_losses = []
    
    # Training loop with stability improvements
    for epoch in range(epochs):
        # Track epoch losses
        epoch_critic_losses = []
        epoch_generator_losses = []
        
        for i in range(len(data_loader)):
            # Adaptive batch size scaling for stability with many categorical variables
            effective_batch_size = batch_size
            if len(categorical_columns) > 10 and epoch < epochs // 3:
                # Start with smaller batches for better stability with many categories
                effective_batch_size = batch_size // 2
            
            # Train critic
            for _ in range(critic_iters):
                # Sample real data
                real_data, real_cond, cond, data_loader_iter = sample_training_data(
                    data_loader, data_loader_iter, categorical_dims, device)
                
                # Generate fake data
                z = torch.randn(effective_batch_size, z_dim, device=device)
                with torch.no_grad():  # Don't track gradients for generator during critic training
                    fake_data = generator_forward(z, cond)
                
                # Calculate critic loss
                real_critic = critic_forward(real_data, real_cond)
                fake_critic = critic_forward(fake_data, cond)
                
                # Gradient penalty with improved stability
                gp = compute_gradient_penalty(real_data, fake_data, cond, device)
                
                # Wasserstein loss with stability term
                # Add a small epsilon to avoid numerical instability
                epsilon = 1e-8
                critic_loss = torch.mean(fake_critic) - torch.mean(real_critic) + lambda_gp * gp + epsilon
                
                # Update critic
                optimizer_c.zero_grad()
                critic_loss.backward()
                
                # Apply gradient clipping to critic with adaptive clipping value
                clip_value = 10.0
                if len(categorical_columns) > 5:
                    # More aggressive clipping with many categorical variables
                    clip_value = 5.0
                torch.nn.utils.clip_grad_norm_(critic_parameters, max_norm=clip_value)
                
                optimizer_c.step()
                
                epoch_critic_losses.append(critic_loss.item())
            
            # Train generator with adaptive training strategy
            # Sample conditions and generate fake data
            _, _, cond, data_loader_iter = sample_training_data(
                data_loader, data_loader_iter, categorical_dims, device)
            z = torch.randn(effective_batch_size, z_dim, device=device)
            fake_data = generator_forward(z, cond)
            
            # Calculate generator loss
            fake_critic = critic_forward(fake_data, cond)
            generator_loss = -torch.mean(fake_critic)
            
            # Add conditional loss if categorical columns exist with adaptive weighting
            if cond_dim > 0:
                # Extract the generated categorical part
                cat_start_idx = continuous_dims + sum(mode_sizes)
                cat_fake_data = fake_data[:, cat_start_idx:]
                
                # Calculate cross-entropy loss for each categorical column
                cond_loss = 0
                current_pos = 0
                
                # Adaptive lambda_cond based on number of categorical columns
                effective_lambda_cond = lambda_cond
                if len(categorical_columns) > 5:
                    # Increase conditional weight with many categories
                    effective_lambda_cond = lambda_cond * (1 + 0.1 * (len(categorical_columns) - 5))
                
                for i, dim in enumerate(categorical_dims):
                    target = cond[:, current_pos:current_pos + dim]
                    pred = cat_fake_data[:, current_pos:current_pos + dim]
                    
                    # Cross entropy between target and prediction with improved stability
                    eps = 1e-8
                    cond_loss += F.binary_cross_entropy(pred + eps, target + eps)
                    current_pos += dim
                
                generator_loss += effective_lambda_cond * cond_loss
            
            # Update generator
            optimizer_g.zero_grad()
            generator_loss.backward()
            
            # Apply gradient clipping to generator with adaptive clipping
            clip_value = 10.0
            if len(categorical_columns) > 5:
                clip_value = 5.0
            torch.nn.utils.clip_grad_norm_(generator_parameters, max_norm=clip_value)
            
            optimizer_g.step()
            
            epoch_generator_losses.append(generator_loss.item())
        
        # Step the learning rate schedulers
        scheduler_g.step()
        scheduler_c.step()
        
        # Calculate average losses for the epoch
        avg_critic_loss = sum(epoch_critic_losses) / len(epoch_critic_losses) if epoch_critic_losses else 0
        avg_generator_loss = sum(epoch_generator_losses) / len(epoch_generator_losses) if epoch_generator_losses else 0
        
        critic_losses.append(avg_critic_loss)
        generator_losses.append(avg_generator_loss)
        
        # Print progress with more detailed diagnostics
        if (epoch + 1) % 10 == 0:
            current_lr_g = scheduler_g.get_last_lr()[0]
            current_lr_c = scheduler_c.get_last_lr()[0]
            
            print(f"Epoch {epoch+1}/{epochs}, G Loss: {avg_generator_loss:.4f}, C Loss: {avg_critic_loss:.4f}, "
                  f"LR_G: {current_lr_g:.6f}, LR_C: {current_lr_c:.6f}")
            
            # Check for NaN values
            if np.isnan(avg_generator_loss) or np.isnan(avg_critic_loss):
                print("NaN values detected - training has diverged. Stopping training.")
                break
            
            # Check if losses are exploding (common with many categorical variables)
            if avg_generator_loss > 1000 or avg_critic_loss > 1000:
                print("Loss explosion detected. Consider reducing learning rate or applying more regularization.")
                # We could break here, but instead we'll try to recover with a learning rate reduction
                for param_group in optimizer_g.param_groups:
                    param_group['lr'] = param_group['lr'] * 0.5
                for param_group in optimizer_c.param_groups:
                    param_group['lr'] = param_group['lr'] * 0.5
            
            # Check if losses are improving
            if epoch > 50 and epoch % 50 == 0:
                # Check if generator loss is improving
                recent_g_loss = np.mean(generator_losses[-20:])
                earlier_g_loss = np.mean(generator_losses[-50:-30])
                
                if abs(recent_g_loss - earlier_g_loss) < 0.01:
                    print("Generator loss has plateaued. Adjusting learning rate...")
                    # Reduce learning rate for generator
                    for param_group in optimizer_g.param_groups:
                        param_group['lr'] = param_group['lr'] * 0.7
                
                # Check if critic loss is improving
                recent_c_loss = np.mean(critic_losses[-20:])
                earlier_c_loss = np.mean(critic_losses[-50:-30])
                
                if abs(recent_c_loss - earlier_c_loss) < 0.01:
                    print("Critic loss has plateaued. Adjusting learning rate...")
                    # Reduce learning rate for critic
                    for param_group in optimizer_c.param_groups:
                        param_group['lr'] = param_group['lr'] * 0.7
    
    # Create a model dictionary to return
    model = {
        'gmms': gmms,
        'n_components': n_components,
        'encoders': encoders,
        'encoded_dims': encoded_dims,
        'continuous_columns': continuous_columns,
        'categorical_columns': categorical_columns,
        'continuous_dims': continuous_dims,
        'mode_sizes': mode_sizes,
        'categorical_dims': categorical_dims,
        'cond_dim': cond_dim,
        'gen_modules': gen_modules,
        'generator_forward': generator_forward,
        'z_dim': z_dim,
        'device': device,
        'date_encoders': date_encoders  # Add date encoders to the model
    }
    
    return model, critic_losses, generator_losses
