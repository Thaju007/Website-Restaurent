"""
Enterprise-ready CTGAN implementation using functional programming approach
Based on the CTGAN paper with PyTorch implementation
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from sklearn.preprocessing import OneHotEncoder

# Constants
BATCH_SIZE = 500
CRITIC_ITERS = 5
LAMBDA_GP = 10
LAMBDA_COND = 1.0
Z_DIM = 128
HIDDEN_DIM = 256
LEARNING_RATE = 2e-4
EPOCHS = 300

# --------- Preprocessing Functions ---------

def fit_gaussian_mixture_models(data, continuous_columns):
    """Fit Gaussian Mixture Models to continuous columns for mode-specific normalization"""
    gmms = {}
    n_components = {}
    
    for column in continuous_columns:
        # Step 1: Use VGM to estimate number of modes
        vgm = BayesianGaussianMixture(
            n_components=10,  # Start with a high number and let VGM prune
            weight_concentration_prior_type="dirichlet_process",
            weight_concentration_prior=0.001,
            n_init=5,
            random_state=42
        )
        vgm.fit(data[column].values.reshape(-1, 1))
        
        # Count effective components (those with non-negligible weights)
        effective_components = np.sum(vgm.weights_ > 0.01)
        effective_components = max(1, effective_components)  # At least 1 component
        n_components[column] = effective_components
        
        # Step 2: Fit a regular GMM with the estimated number of components
        gmm = GaussianMixture(
            n_components=effective_components,
            n_init=5,
            random_state=42
        )
        gmm.fit(data[column].values.reshape(-1, 1))
        gmms[column] = gmm
    
    return gmms, n_components

def normalize_continuous_columns(data, continuous_columns, gmms):
    """
    Apply mode-specific normalization to continuous columns
    Returns normalized data and mode indicators
    """
    normalized_data = data.copy()
    mode_indicators = {}
    
    for column in continuous_columns:
        gmm = gmms[column]
        values = data[column].values.reshape(-1, 1)
        
        # Step 3: Compute probability densities for each mode
        probs = gmm.predict_proba(values)
        
        # Step 4: Sample modes based on these probabilities
        modes = np.zeros_like(probs)
        selected_modes = np.argmax(probs, axis=1)
        modes[np.arange(len(selected_modes)), selected_modes] = 1
        
        # Normalize within the selected mode
        means = gmm.means_.reshape(-1)
        stds = np.sqrt(gmm.covariances_).reshape(-1)
        
        normalized_values = np.zeros(len(values))
        for mode_idx in range(gmm.n_components):
            mode_filter = (selected_modes == mode_idx)
            if np.any(mode_filter):
                normalized_values[mode_filter] = ((values[mode_filter].reshape(-1) - means[mode_idx]) / 
                                                 (4 * stds[mode_idx]))
        
        normalized_data[column] = normalized_values
        mode_indicators[column] = modes
    
    return normalized_data, mode_indicators

def fit_categorical_encoders(data, categorical_columns):
    """Fit one-hot encoders for categorical columns"""
    encoders = {}
    encoded_dims = {}
    
    for column in categorical_columns:
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        encoder.fit(data[column].values.reshape(-1, 1))
        encoders[column] = encoder
        encoded_dims[column] = len(encoder.categories_[0])
    
    return encoders, encoded_dims

def transform_data(data, continuous_columns, categorical_columns, gmms, encoders):
    """Transform data for model input"""
    # Normalize continuous columns
    normalized_data, mode_indicators = normalize_continuous_columns(data, continuous_columns, gmms)
    
    # Transform categorical columns using one-hot encoding
    transformed_data = []
    
    # First, add normalized continuous data
    for column in continuous_columns:
        transformed_data.append(normalized_data[column].values.reshape(-1, 1))
    
    # Next, add mode indicators for continuous data
    for column in continuous_columns:
        transformed_data.append(mode_indicators[column])
    
    # Finally, add one-hot encoded categorical data
    for column in categorical_columns:
        encoder = encoders[column]
        encoded = encoder.transform(data[column].values.reshape(-1, 1))
        transformed_data.append(encoded)
    
    # Concatenate all components
    return np.hstack(transformed_data)

def inverse_transform(generated_data, continuous_columns, categorical_columns, gmms, encoders, n_continuous):
    """Convert generated data back to original format"""
    data_dict = {}
    
    # Process continuous columns
    current_idx = 0
    cont_normalized_data = {}
    mode_indicators = {}
    
    # Extract normalized values
    for i, column in enumerate(continuous_columns):
        cont_normalized_data[column] = generated_data[:, current_idx].numpy()
        current_idx += 1
    
    # Extract mode indicators
    for i, column in enumerate(continuous_columns):
        gmm = gmms[column]
        n_modes = gmm.n_components
        mode_indicators[column] = generated_data[:, current_idx:current_idx + n_modes].numpy()
        current_idx += n_modes
    
    # Denormalize continuous data
    for column in continuous_columns:
        gmm = gmms[column]
        normalized_values = cont_normalized_data[column]
        modes = np.argmax(mode_indicators[column], axis=1)
        
        means = gmm.means_.reshape(-1)
        stds = np.sqrt(gmm.covariances_).reshape(-1)
        
        denormalized = np.zeros_like(normalized_values)
        for mode_idx in range(gmm.n_components):
            mode_filter = (modes == mode_idx)
            if np.any(mode_filter):
                denormalized[mode_filter] = (normalized_values[mode_filter] * 4 * stds[mode_idx]) + means[mode_idx]
        
        data_dict[column] = denormalized
    
    # Process categorical columns
    for column in categorical_columns:
        encoder = encoders[column]
        n_categories = len(encoder.categories_[0])
        
        # Extract one-hot encoded data for this column
        encoded_data = generated_data[:, current_idx:current_idx + n_categories].numpy()
        current_idx += n_categories
        
        # Convert from one-hot back to categorical
        categorical_data = np.argmax(encoded_data, axis=1)
        
        # Map from numeric indices back to original categories
        original_categories = encoder.categories_[0]
        data_dict[column] = np.array([original_categories[idx] for idx in categorical_data])
    
    return pd.DataFrame(data_dict)

# --------- GAN Model Functions ---------

def gumbel_softmax(logits, temperature=0.2):
    """Gumbel-Softmax activation function"""
    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)
    y = logits + gumbel_noise
    return F.softmax(y / temperature, dim=-1)

def generator_network(z, cond, continuous_dims, mode_sizes, categorical_dims):
    """
    Generator network as described in the paper
    z: random noise
    cond: conditional vector for controlling discrete outputs
    """
    # Combine noise and condition
    h0 = torch.cat([z, cond], dim=1)
    input_dim = z.size(1) + cond.size(1)
    
    # First hidden layer with skip connection
    fc1 = nn.Linear(input_dim, HIDDEN_DIM)
    bn1 = nn.BatchNorm1d(HIDDEN_DIM)
    h1 = torch.cat([h0, F.relu(bn1(fc1(h0)))], dim=1)
    
    # Second hidden layer with skip connection
    fc2 = nn.Linear(input_dim + HIDDEN_DIM, HIDDEN_DIM)
    bn2 = nn.BatchNorm1d(HIDDEN_DIM)
    h2 = torch.cat([h1, F.relu(bn2(fc2(h1)))], dim=1)
    
    last_dim = input_dim + HIDDEN_DIM * 2
    
    # Output layers
    output_parts = []
    
    # Generate continuous normalized values using tanh
    for i in range(continuous_dims):
        fc_alpha = nn.Linear(last_dim, 1)
        alpha = torch.tanh(fc_alpha(h2))
        output_parts.append(alpha)
    
    # Generate mode indicators using gumbel softmax
    for size in mode_sizes:
        fc_beta = nn.Linear(last_dim, size)
        beta = gumbel_softmax(fc_beta(h2), temperature=0.2)
        output_parts.append(beta)
    
    # Generate discrete values using gumbel softmax
    for size in categorical_dims:
        fc_d = nn.Linear(last_dim, size)
        d = gumbel_softmax(fc_d(h2), temperature=0.2)
        output_parts.append(d)
    
    # Concatenate all outputs
    return torch.cat(output_parts, dim=1)

def critic_network(x, cond, pac_size=10):
    """
    Critic network as described in the paper
    Implements PacGAN with given pac_size
    """
    # Reshape inputs for pac
    batch_size = x.size(0) // pac_size
    x = x.view(batch_size, pac_size * x.size(1))
    cond = cond.view(batch_size, pac_size * cond.size(1))
    
    # Combine inputs and condition
    h0 = torch.cat([x, cond], dim=1)
    
    # First hidden layer with dropout and leaky ReLU
    fc1 = nn.Linear(h0.size(1), HIDDEN_DIM)
    h1 = F.leaky_relu(fc1(h0), 0.2)
    h1 = F.dropout(h1, 0.5, training=True)
    
    # Second hidden layer with dropout and leaky ReLU
    fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)
    h2 = F.leaky_relu(fc2(h1), 0.2)
    h2 = F.dropout(h2, 0.5, training=True)
    
    # Output layer
    fc3 = nn.Linear(HIDDEN_DIM, 1)
    return fc3(h2)

def compute_gradient_penalty(critic, real_data, fake_data, cond, device):
    """Calculate gradient penalty for WGAN-GP"""
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, device=device)
    
    # Interpolate between real and fake data
    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)
    
    # Calculate critic output for interpolated data
    disc_interpolates = critic_network(interpolates, cond)
    
    # Calculate gradients
    gradients = torch.autograd.grad(
        outputs=disc_interpolates, 
        inputs=interpolates,
        grad_outputs=torch.ones_like(disc_interpolates),
        create_graph=True, 
        retain_graph=True
    )[0]
    
    # Calculate gradient penalty
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()
    
    return gradient_penalty

def sample_training_data(data_loader, categorical_dims, device):
    """Sample training data with training-by-sampling method"""
    # Get a batch of real data
    try:
        real_data, real_cond = next(data_loader_iter)
    except:
        data_loader_iter = iter(data_loader)
        real_data, real_cond = next(data_loader_iter)
    
    real_data = real_data.to(device)
    real_cond = real_cond.to(device)
    
    # Create conditional vector for generator
    batch_size = real_data.size(0)
    cond = torch.zeros_like(real_cond)
    
    # For each sample, randomly select one discrete column
    for i in range(batch_size):
        # Randomly select a categorical column
        if len(categorical_dims) > 0:
            selected_col = np.random.randint(len(categorical_dims))
            
            # Create probability distribution based on log(frequency)
            # For simplicity, we use uniform sampling here
            # In a full implementation, you'd use the actual frequencies
            col_start = sum(categorical_dims[:selected_col])
            col_size = categorical_dims[selected_col]
            col_val = np.random.randint(col_size)
            
            # Set the corresponding element to 1
            cond[i, col_start + col_val] = 1
    
    return real_data, real_cond, cond

def create_noise(batch_size, z_dim, device):
    """Create random noise for generator input"""
    return torch.randn(batch_size, z_dim, device=device)

def train_ctgan(data, continuous_columns, categorical_columns, device='cuda'):
    """Train the CTGAN model"""
    # Preprocess data
    gmms, n_components = fit_gaussian_mixture_models(data, continuous_columns)
    encoders, encoded_dims = fit_categorical_encoders(data, categorical_columns)
    
    transformed_data = transform_data(data, continuous_columns, categorical_columns, gmms, encoders)
    
    # Prepare dimensions information
    continuous_dims = len(continuous_columns)
    mode_sizes = [n_components[col] for col in continuous_columns]
    categorical_dims = [encoded_dims[col] for col in categorical_columns]
    
    # Calculate conditional vector size (sum of all categorical dimensions)
    cond_dim = sum(categorical_dims)
    
    # Create dataset and dataloader
    tensor_x = torch.Tensor(transformed_data)
    
    # Create conditional vectors (one-hot encoding of categorical columns)
    cond_data = np.zeros((len(tensor_x), cond_dim))
    current_pos = 0
    for i, col in enumerate(categorical_columns):
        col_data = transformed_data[:, -sum(categorical_dims) + current_pos:
                                    -sum(categorical_dims) + current_pos + categorical_dims[i]]
        cond_data[:, current_pos:current_pos + categorical_dims[i]] = col_data
        current_pos += categorical_dims[i]
    
    tensor_cond = torch.Tensor(cond_data)
    dataset = TensorDataset(tensor_x, tensor_cond)
    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    data_loader_iter = iter(data_loader)
    
    # Store optimizers and modules in dicts to ensure proper tracking
    gen_modules = {}
    critic_modules = {}
    
    # Generator modules
    gen_modules['fc1'] = nn.Linear(Z_DIM + cond_dim, HIDDEN_DIM).to(device)
    gen_modules['bn1'] = nn.BatchNorm1d(HIDDEN_DIM).to(device)
    gen_modules['fc2'] = nn.Linear(Z_DIM + cond_dim + HIDDEN_DIM, HIDDEN_DIM).to(device)
    gen_modules['bn2'] = nn.BatchNorm1d(HIDDEN_DIM).to(device)
    
    last_dim = Z_DIM + cond_dim + HIDDEN_DIM * 2
    
    # Output layers for generator
    for i in range(continuous_dims):
        gen_modules[f'fc_alpha_{i}'] = nn.Linear(last_dim, 1).to(device)
    
    for i, size in enumerate(mode_sizes):
        gen_modules[f'fc_beta_{i}'] = nn.Linear(last_dim, size).to(device)
    
    for i, size in enumerate(categorical_dims):
        gen_modules[f'fc_d_{i}'] = nn.Linear(last_dim, size).to(device)
    
    # Critic modules
    input_dim = tensor_x.size(1)
    critic_modules['fc1'] = nn.Linear(10 * input_dim + 10 * cond_dim, HIDDEN_DIM).to(device)
    critic_modules['fc2'] = nn.Linear(HIDDEN_DIM, HIDDEN_DIM).to(device)
    critic_modules['fc3'] = nn.Linear(HIDDEN_DIM, 1).to(device)
    
    # Collect parameters for optimizers
    generator_parameters = []
    for module in gen_modules.values():
        generator_parameters.extend(module.parameters())
    
    critic_parameters = []
    for module in critic_modules.values():
        critic_parameters.extend(module.parameters())
    
    # Create optimizers with proper learning rates
    optimizer_g = optim.Adam(generator_parameters, lr=LEARNING_RATE, betas=(0.5, 0.9))
    optimizer_c = optim.Adam(critic_parameters, lr=LEARNING_RATE, betas=(0.5, 0.9))
    
    # Define a generator function that uses the modules
    def generator_forward(z, cond):
        # Combine noise and condition
        h0 = torch.cat([z, cond], dim=1)
        
        # First hidden layer with skip connection
        h1_part = F.relu(gen_modules['bn1'](gen_modules['fc1'](h0)))
        h1 = torch.cat([h0, h1_part], dim=1)
        
        # Second hidden layer with skip connection
        h2_part = F.relu(gen_modules['bn2'](gen_modules['fc2'](h1)))
        h2 = torch.cat([h1, h2_part], dim=1)
        
        # Output layers
        output_parts = []
        
        # Generate continuous normalized values using tanh
        for i in range(continuous_dims):
            alpha = torch.tanh(gen_modules[f'fc_alpha_{i}'](h2))
            output_parts.append(alpha)
        
        # Generate mode indicators using gumbel softmax
        for i, size in enumerate(mode_sizes):
            beta = gumbel_softmax(gen_modules[f'fc_beta_{i}'](h2), temperature=0.2)
            output_parts.append(beta)
        
        # Generate discrete values using gumbel softmax
        for i, size in enumerate(categorical_dims):
            d = gumbel_softmax(gen_modules[f'fc_d_{i}'](h2), temperature=0.2)
            output_parts.append(d)
        
        # Concatenate all outputs
        return torch.cat(output_parts, dim=1)
    
    # Define a critic function that uses the modules
    def critic_forward(x, cond, pac_size=10):
        # Reshape inputs for pac
        batch_size = x.size(0) // pac_size
        x = x.view(batch_size, pac_size * x.size(1))
        cond = cond.view(batch_size, pac_size * cond.size(1))
        
        # Combine inputs and condition
        h0 = torch.cat([x, cond], dim=1)
        
        # First hidden layer with dropout and leaky ReLU
        h1 = F.leaky_relu(critic_modules['fc1'](h0), 0.2)
        h1 = F.dropout(h1, 0.5, training=True)
        
        # Second hidden layer with dropout and leaky ReLU
        h2 = F.leaky_relu(critic_modules['fc2'](h1), 0.2)
        h2 = F.dropout(h2, 0.5, training=True)
        
        # Output layer
        return critic_modules['fc3'](h2)
    
    # Function to compute gradient penalty
    def compute_gradient_penalty(real_data, fake_data, cond, device):
        batch_size = real_data.size(0)
        alpha = torch.rand(batch_size, 1, device=device)
        
        # Interpolate between real and fake data
        alpha = alpha.expand(real_data.size())
        interpolates = alpha * real_data + (1 - alpha) * fake_data
        interpolates.requires_grad_(True)
        
        # Calculate critic output for interpolated data
        disc_interpolates = critic_forward(interpolates, cond)
        
        # Calculate gradients
        gradients = torch.autograd.grad(
            outputs=disc_interpolates, 
            inputs=interpolates,
            grad_outputs=torch.ones_like(disc_interpolates, device=device),
            create_graph=True, 
            retain_graph=True
        )[0]
        
        # Calculate gradient penalty
        gradients = gradients.view(batch_size, -1)
        gradient_norm = gradients.norm(2, dim=1)
        gradient_penalty = ((gradient_norm - 1) ** 2).mean()
        
        return gradient_penalty
    
    # Modified training-by-sampling function
    def sample_training_data(data_loader, data_loader_iter, categorical_dims, device):
        # Get a batch of real data
        try:
            real_data, real_cond = next(data_loader_iter)
        except StopIteration:
            data_loader_iter = iter(data_loader)
            real_data, real_cond = next(data_loader_iter)
        
        real_data = real_data.to(device)
        real_cond = real_cond.to(device)
        
        # Create conditional vector for generator
        batch_size = real_data.size(0)
        cond = torch.zeros_like(real_cond)
        
        # For each sample, randomly select one discrete column
        if len(categorical_dims) > 0:
            for i in range(batch_size):
                # Randomly select a categorical column
                selected_col = np.random.randint(len(categorical_dims))
                
                # Create probability distribution based on log(frequency)
                # For simplicity, we use uniform sampling here
                col_start = sum(categorical_dims[:selected_col])
                col_size = categorical_dims[selected_col]
                col_val = np.random.randint(col_size)
                
                # Set the corresponding element to 1
                cond[i, col_start + col_val] = 1
        
        return real_data, real_cond, cond, data_loader_iter
    
    # Initialize tracking variables
    critic_losses = []
    generator_losses = []
    
    # Apply weight initialization
    for module in gen_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0.0)
    
    for module in critic_modules.values():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0.0)
    
    # Training loop with stability improvements
    for epoch in range(EPOCHS):
        # Track epoch losses
        epoch_critic_losses = []
        epoch_generator_losses = []
        
        for i in range(len(data_loader)):
            # Train critic
            for _ in range(CRITIC_ITERS):
                # Sample real data
                real_data, real_cond, cond, data_loader_iter = sample_training_data(
                    data_loader, data_loader_iter, categorical_dims, device)
                
                # Generate fake data
                z = torch.randn(BATCH_SIZE, Z_DIM, device=device)
                with torch.no_grad():  # Don't track gradients for generator during critic training
                    fake_data = generator_forward(z, cond)
                
                # Calculate critic loss
                real_critic = critic_forward(real_data, real_cond)
                fake_critic = critic_forward(fake_data, cond)
                
                # Gradient penalty
                gp = compute_gradient_penalty(real_data, fake_data, cond, device)
                
                # Wasserstein loss with stability term
                critic_loss = torch.mean(fake_critic) - torch.mean(real_critic) + LAMBDA_GP * gp
                
                # Update critic
                optimizer_c.zero_grad()
                critic_loss.backward()
                
                # Apply gradient clipping to critic
                torch.nn.utils.clip_grad_norm_(critic_parameters, max_norm=10.0)
                optimizer_c.step()
                
                epoch_critic_losses.append(critic_loss.item())
            
            # Train generator
            # Sample conditions and generate fake data
            _, _, cond, data_loader_iter = sample_training_data(
                data_loader, data_loader_iter, categorical_dims, device)
            z = torch.randn(BATCH_SIZE, Z_DIM, device=device)
            fake_data = generator_forward(z, cond)
            
            # Calculate generator loss
            fake_critic = critic_forward(fake_data, cond)
            generator_loss = -torch.mean(fake_critic)
            
            # Add conditional loss if categorical columns exist
            if cond_dim > 0:
                # Extract the generated categorical part
                cat_start_idx = continuous_dims + sum(mode_sizes)
                cat_fake_data = fake_data[:, cat_start_idx:]
                
                # Calculate cross-entropy loss for each categorical column
                cond_loss = 0
                current_pos = 0
                for i, dim in enumerate(categorical_dims):
                    target = cond[:, current_pos:current_pos + dim]
                    pred = cat_fake_data[:, current_pos:current_pos + dim]
                    
                    # Cross entropy between target and prediction
                    cond_loss += F.binary_cross_entropy(pred + 1e-8, target + 1e-8)
                    current_pos += dim
                
                generator_loss += LAMBDA_COND * cond_loss
            
            # Update generator
            optimizer_g.zero_grad()
            generator_loss.backward()
            
            # Apply gradient clipping to generator
            torch.nn.utils.clip_grad_norm_(generator_parameters, max_norm=10.0)
            optimizer_g.step()
            
            epoch_generator_losses.append(generator_loss.item())
        
        # Calculate average losses for the epoch
        avg_critic_loss = sum(epoch_critic_losses) / len(epoch_critic_losses) if epoch_critic_losses else 0
        avg_generator_loss = sum(epoch_generator_losses) / len(epoch_generator_losses) if epoch_generator_losses else 0
        
        critic_losses.append(avg_critic_loss)
        generator_losses.append(avg_generator_loss)
        
        # Print progress
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS}, G Loss: {avg_generator_loss:.4f}, C Loss: {avg_critic_loss:.4f}")
            
            # Check for NaN values
            if np.isnan(avg_generator_loss) or np.isnan(avg_critic_loss):
                print("NaN values detected - training has diverged. Stopping training.")
                break
            
            # Check if losses are improving
            if epoch > 50 and epoch % 50 == 0:
                # Check if generator loss is improving
                recent_g_loss = np.mean(generator_losses[-20:])
                earlier_g_loss = np.mean(generator_losses[-50:-30])
                
                if abs(recent_g_loss - earlier_g_loss) < 0.01:
                    print("Generator loss has plateaued. Consider adjusting learning rate or model parameters.")
                
                # Check if critic loss is improving
                recent_c_loss = np.mean(critic_losses[-20:])
                earlier_c_loss = np.mean(critic_losses[-50:-30])
                
                if abs(recent_c_loss - earlier_c_loss) < 0.01:
                    print("Critic loss has plateaued. Consider adjusting learning rate or model parameters.")
    
    # Create a model dictionary to return
    model = {
        'gmms': gmms,
        'n_components': n_components,
        'encoders': encoders,
        'encoded_dims': encoded_dims,
        'continuous_columns': continuous_columns,
        'categorical_columns': categorical_columns,
        'continuous_dims': continuous_dims,
        'mode_sizes': mode_sizes,
        'categorical_dims': categorical_dims,
        'cond_dim': cond_dim,
        'gen_modules': gen_modules,
        'generator_forward': generator_forward,
        'z_dim': Z_DIM,
        'device': device
    }
    
    return model, critic_losses, generator_losses

def generate_samples(model, n_samples):
    """Generate synthetic samples using the trained model"""
    # Unpack model components
    gmms = model['gmms']
    encoders = model['encoders']
    continuous_columns = model['continuous_columns']
    categorical_columns = model['categorical_columns']
    continuous_dims = model['continuous_dims']
    mode_sizes = model['mode_sizes']
    categorical_dims = model['categorical_dims']
    cond_dim = model['cond_dim']
    gen_modules = model['gen_modules']
    generator_forward = model['generator_forward']
    z_dim = model['z_dim']
    device = model['device']
    
    # Create random noise
    z = torch.randn(n_samples, z_dim, device=device)
    
    # Create random conditions (for enterprise use, you may want to control this)
    cond = torch.zeros(n_samples, cond_dim, device=device)
    if len(categorical_dims) > 0:
        for i in range(n_samples):
            for j, dim in enumerate(categorical_dims):
                col_start = sum(categorical_dims[:j])
                col_val = np.random.randint(dim)
                cond[i, col_start + col_val] = 1
    
    # Generate data
    with torch.no_grad():
        fake_data = generator_forward(z, cond)
    
    # Convert to original format
    return inverse_transform(fake_data, continuous_columns, categorical_columns, 
                           gmms, encoders, continuous_dims)

# Example usage
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("your_data.csv")
    
    # Define continuous and categorical columns
    continuous_columns = ["age", "income", "height", "weight"]
    categorical_columns = ["gender", "education", "occupation"]
    
    # Train model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, critic_losses, generator_losses = train_ctgan(data, continuous_columns, categorical_columns, device)
    
    # Plot training losses
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 5))
    plt.plot(critic_losses, label='Critic Loss')
    plt.plot(generator_losses, label='Generator Loss')
    plt.legend()
    plt.title('CTGAN Training Losses')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig('training_losses.png')
    
    # Generate synthetic data
    synthetic_data = generate_samples(model, 1000)
    
    # Save synthetic data
    synthetic_data.to_csv("synthetic_data.csv", index=False)
